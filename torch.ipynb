{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import datetime as dt\n",
    "from datetime import timedelta\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputLayer(nn.Module):\n",
    "    def __init__(self, height, width):\n",
    "        super().__init__()\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        \n",
    "    def forward(self, u_vert, u_hori, rho):\n",
    "        # use feq as input field\n",
    "        # todo: make this layer be also learnable\n",
    "        \n",
    "        # feq(x,v) = C[v] * rho * (1 + 3 v.u + 4.5 (v.u)^2 - 1.5 u^2)\n",
    "        feq = torch.empty((3, 3, self.height, self.width), dtype=torch.float32, device=device)\n",
    "        C = torch.tensor([[1.0/36.0, 1.0/9.0, 1.0/36.0], [1.0/9.0, 4.0/9.0, 1.0/9.0], [1.0/36.0, 1.0/9.0, 1.0/36.0]], dtype=torch.float32, device=device)\n",
    "        u_squared = u_vert ** 2 + u_hori ** 2\n",
    "        \n",
    "        for dr in range(-1, 2):\n",
    "            for dc in range(-1, 2):\n",
    "                v_dot_u = dr * u_vert + dc * u_hori\n",
    "                feq[dr+1, dc+1, :, :] = C[dr+1,dc+1] * rho * (1.0 + 3.0 * v_dot_u + 4.5 * v_dot_u * v_dot_u - 1.5 * u_squared)\n",
    "                \n",
    "        return feq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1000)\n",
      "tensor(-0.3000)\n",
      "tensor(0.8000)\n"
     ]
    }
   ],
   "source": [
    "# test for InputLayer\n",
    "# # Later, I'll check the immutability of the InputLayer\n",
    "#  - it must put out the same tensor even after the weights are updated\n",
    "input_layer_u_vert = torch.tensor([[0.1]], dtype=torch.float32)\n",
    "input_layer_u_hori = torch.tensor([[-0.3]], dtype=torch.float32)\n",
    "input_layer_rho = torch.tensor([[0.8]], dtype=torch.float32)\n",
    "\n",
    "input_layer = InputLayer(1, 1).to(device)\n",
    "feq = input_layer(input_layer_u_vert, input_layer_u_hori, input_layer_rho)\n",
    "print(torch.sum(feq[2,:,0,0] - feq[0,:,0,0]) / torch.sum(feq[:,:,0,0])) # u_vert\n",
    "print(torch.sum(feq[:,2,0,0] - feq[:,0,0,0]) / torch.sum(feq[:,:,0,0])) # u_hori\n",
    "print(torch.sum(feq[:,:,0,0])) # rho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StreamingLayer(nn.Module):\n",
    "    def __init__(self, in_height, in_width):\n",
    "        super().__init__()\n",
    "        self.in_height = in_height\n",
    "        self.in_width = in_width\n",
    "        \n",
    "        self.w0 = nn.Parameter(torch.zeros((in_height-2, in_width-2), dtype=torch.float32, device=device))\n",
    "        self.w1 = nn.Parameter(torch.ones((in_height-2, in_width-2), dtype=torch.float32, device=device))\n",
    "        \n",
    "    def forward(self, f_prev):\n",
    "        # f(x,v) = w0(x,v) + w1(x,v) * f_prev(x-v,v)\n",
    "        \n",
    "        f = torch.empty((3, 3, self.in_height-2, self.in_width-2), dtype=torch.float32, device=device)\n",
    "        for dr in range(-1, 2):\n",
    "            for dc in range(-1, 2):\n",
    "                f[dr+1,dc+1,:,:] = self.w0 + self.w1 * f_prev[dr+1,dc+1,1-dr:self.in_height-1-dr,1-dc:self.in_width-1-dc]\n",
    "        \n",
    "        return f\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 0., 9.]],\n",
      "\n",
      "         [[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 8., 0.]],\n",
      "\n",
      "         [[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [7., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.],\n",
      "          [0., 0., 6.],\n",
      "          [0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.],\n",
      "          [0., 5., 0.],\n",
      "          [0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.],\n",
      "          [4., 0., 0.],\n",
      "          [0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 3.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 0., 0.]],\n",
      "\n",
      "         [[0., 2., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 0., 0.]],\n",
      "\n",
      "         [[1., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 0., 0.]]]], dtype=torch.float64)\n",
      "tensor([[[[9.]],\n",
      "\n",
      "         [[8.]],\n",
      "\n",
      "         [[7.]]],\n",
      "\n",
      "\n",
      "        [[[6.]],\n",
      "\n",
      "         [[5.]],\n",
      "\n",
      "         [[4.]]],\n",
      "\n",
      "\n",
      "        [[[3.]],\n",
      "\n",
      "         [[2.]],\n",
      "\n",
      "         [[1.]]]], grad_fn=<CopySlices>)\n"
     ]
    }
   ],
   "source": [
    "# test for StreamingLayer\n",
    "input_f_prev = torch.zeros((3, 3, 3, 3), dtype=torch.float64)\n",
    "input_f_prev[2,2,0,0] = 1.0\n",
    "input_f_prev[2,1,0,1] = 2.0\n",
    "input_f_prev[2,0,0,2] = 3.0\n",
    "input_f_prev[1,2,1,0] = 4.0\n",
    "input_f_prev[1,1,1,1] = 5.0\n",
    "input_f_prev[1,0,1,2] = 6.0\n",
    "input_f_prev[0,2,2,0] = 7.0\n",
    "input_f_prev[0,1,2,1] = 8.0\n",
    "input_f_prev[0,0,2,2] = 9.0\n",
    "print(input_f_prev)\n",
    "\n",
    "streaming_layer = StreamingLayer(3, 3).to(device)\n",
    "f_streamed = streaming_layer(input_f_prev)\n",
    "print(f_streamed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CollidingLayer(nn.Module):\n",
    "    def __init__(self, in_height, in_width):\n",
    "        super().__init__()\n",
    "        self.in_height = in_height\n",
    "        self.in_width = in_width\n",
    "        \n",
    "        self.w1 = nn.Parameter(torch.full((in_height, in_width), fill_value=3.0, dtype=torch.float32, device=device), requires_grad=True)\n",
    "        self.w2 = nn.Parameter(torch.full((in_height, in_width), fill_value=0.0, dtype=torch.float32, device=device), requires_grad=True)\n",
    "        self.w3 = nn.Parameter(torch.full((in_height, in_width), fill_value=4.5, dtype=torch.float32, device=device), requires_grad=True)\n",
    "        self.w4 = nn.Parameter(torch.full((in_height, in_width), fill_value=-1.5, dtype=torch.float32, device=device), requires_grad=True)\n",
    "        \n",
    "    def forward(self, f_prev):\n",
    "        # feq(x,v) = C(v) * rho(x) * (1 + w1(x,v) v.u(x) + w2(x,v) vXu(x) + w3(x,v) (v.u(x))^2 + w4(x,v) u(x)^2)\n",
    "        # f = (f_prev + feq) / 2\n",
    "        C = torch.tensor([[1.0/36.0, 1.0/9.0, 1.0/36.0], [1.0/9.0, 4.0/9.0, 1.0/9.0], [1.0/36.0, 1.0/9.0, 1.0/36.0]], dtype=torch.float32, device=device)\n",
    "        \n",
    "        rho = torch.sum(f_prev, dim=(0,1))\n",
    "        u_vert = (f_prev[2,0,:,:] + f_prev[2,1,:,:] + f_prev[2,2,:,:] - f_prev[0,0,:,:] - f_prev[0,1,:,:] - f_prev[0,2,:,:]) / rho\n",
    "        u_hori = (f_prev[0,2,:,:] + f_prev[1,2,:,:] + f_prev[2,2,:,:] - f_prev[0,0,:,:] - f_prev[1,0,:,:] - f_prev[2,0,:,:]) / rho\n",
    "        u_squared = u_vert ** 2 + u_hori ** 2\n",
    "        feq = torch.empty((3, 3, self.in_height, self.in_width), dtype=torch.float32, device=device)\n",
    "        \n",
    "        for dr in range(-1, 2):\n",
    "            for dc in range(-1, 2):\n",
    "                v_dot_u = dr * u_vert + dc * u_hori\n",
    "                vXu = dr * u_hori - dc * u_vert\n",
    "                feq[dr+1, dc+1, :, :] = C[dr+1,dc+1] * rho * (1.0 + (self.w1 + self.w3 * v_dot_u) * v_dot_u + self.w2 * vXu + self.w4 * u_squared)\n",
    "        \n",
    "        return (f_prev + feq) / 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(45., grad_fn=<SumBackward0>)\n",
      "tensor(-0.4000, grad_fn=<DivBackward0>)\n",
      "-0.4\n",
      "tensor(-0.1333, grad_fn=<DivBackward0>)\n",
      "-0.13333333333333333\n"
     ]
    }
   ],
   "source": [
    "# test for CollidingLayer\n",
    "input_f = torch.tensor([[[[9., 9.]],[[8., 8.]],[[7., 7.]]],[[[6., 6.]],[[5., 5.]],[[4., 4.]]],[[[3., 3.]],[[2., 2.]],[[1., 1.]]]], dtype=torch.float32)\n",
    "colliding_layer = CollidingLayer(1, 2)\n",
    "collided_field = colliding_layer(input_f)\n",
    "\n",
    "print(torch.sum(collided_field[:,:,0,0])) # rho 45.0\n",
    "print((torch.sum(collided_field[2,:,0,0]) - torch.sum(collided_field[0,:,0,0])) / torch.sum(collided_field[:,:,0,0])) # u_vert\n",
    "print((1.+2.+3.-9.-8.-7.)/45.)\n",
    "print((torch.sum(collided_field[:,2,0,0]) - torch.sum(collided_field[:,0,0,0])) / torch.sum(collided_field[:,:,0,0])) # u_hori\n",
    "print((1.+4.+7.-3.-6.-9.)/45.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutputLayer(nn.Module):\n",
    "    def __init__(self, height, width):\n",
    "        super().__init__()\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        \n",
    "    def forward(self, f):\n",
    "        rho = torch.sum(f, dim=(0,1))\n",
    "        u_vert = (f[2,0,:,:] + f[2,1,:,:] + f[2,2,:,:] - f[0,0,:,:] - f[0,1,:,:] - f[0,2,:,:]) / rho\n",
    "        u_hori = (f[0,2,:,:] + f[1,2,:,:] + f[2,2,:,:] - f[0,0,:,:] - f[1,0,:,:] - f[2,0,:,:]) / rho\n",
    "        return u_vert, u_hori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LBM(nn.Module):\n",
    "    margin = 3\n",
    "    \n",
    "    def __init__(self, height, width):\n",
    "        super(LBM, self).__init__()\n",
    "        self.input_layer = InputLayer(height, width)\n",
    "        self.streaming1_layer = StreamingLayer(height, width)\n",
    "        self.colliding1_layer = CollidingLayer(height-2, width-2)\n",
    "        self.streaming2_layer = StreamingLayer(height-2, width-2)\n",
    "        self.colliding2_layer = CollidingLayer(height-4, width-4)\n",
    "        self.streaming3_layer = StreamingLayer(height-4, width-4)\n",
    "        self.output_layer = OutputLayer(height-6, width-6)\n",
    "        \n",
    "    def forward(self, u_vert, u_hori, rho):\n",
    "        f = self.input_layer(u_vert, u_hori, rho)\n",
    "        f = self.streaming1_layer(f)\n",
    "        f = self.colliding1_layer(f)\n",
    "        f = self.streaming2_layer(f)\n",
    "        f = self.colliding2_layer(f)\n",
    "        f = self.streaming3_layer(f)\n",
    "        u_vert_out, u_hori_out = self.output_layer(f)\n",
    "        return u_vert_out, u_hori_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_func(u_vert_pred, u_hori_pred, u_vert_target, u_hori_target):\n",
    "    # MSE\n",
    "    return torch.sum(torch.abs(u_vert_pred - u_vert_target) + torch.abs(u_hori_pred - u_hori_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeatherDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, src_dir, datetimes, transform, target_transform, margin):\n",
    "        self.src_dir = src_dir\n",
    "        self.u_vert_paths = []\n",
    "        self.u_hori_paths = []\n",
    "        self.rho_paths = []\n",
    "        self.u_vert_target_paths = []\n",
    "        self.u_hori_target_paths = []\n",
    "        self.margin = margin # this value is the number of streaming layers that reduce the border cells.\n",
    "        # eg. ...                          xxx\n",
    "        #     ...  --(Streaming Layer)-->  x.x\n",
    "        #     ...                          xxx\n",
    "        for datetime in datetimes:\n",
    "            self.u_vert_paths.append(os.path.join(src_dir, datetime.strftime(\"u_vert_%Y%m%d%H.npy\")))\n",
    "            self.u_hori_paths.append(os.path.join(src_dir, datetime.strftime(\"u_hori_%Y%m%d%H.npy\")))\n",
    "            self.rho_paths.append(os.path.join(src_dir, datetime.strftime(\"pressure_%Y%m%d%H.npy\")))\n",
    "            datetime += timedelta(hours=3)\n",
    "            self.u_vert_target_paths.append(os.path.join(src_dir, datetime.strftime('u_vert_%Y%m%d%H.npy')))\n",
    "            self.u_hori_target_paths.append(os.path.join(src_dir, datetime.strftime('u_hori_%Y%m%d%H.npy')))\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.u_vert_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        u_vert = torch.from_numpy(np.load(self.u_vert_paths[idx]))\n",
    "        u_hori = torch.from_numpy(np.load(self.u_hori_paths[idx]))\n",
    "        rho = torch.from_numpy(np.load(self.rho_paths[idx]))\n",
    "        u_vert_target = torch.from_numpy(np.load(self.u_vert_target_paths[idx]))\n",
    "        u_hori_target = torch.from_numpy(np.load(self.u_hori_target_paths[idx]))\n",
    "        u_vert, u_hori, rho = self.transform(u_vert, u_hori, rho)\n",
    "        u_vert_target, u_hori_target = self.target_transform(u_vert_target, u_hori_target, self.margin)\n",
    "        return {\n",
    "            \"u_vert\": u_vert,\n",
    "            \"u_hori\": u_hori,\n",
    "            \"rho\": rho,\n",
    "            \"u_vert_target\": u_vert_target,\n",
    "            \"u_hori_target\": u_hori_target\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(u_vert, u_hori, rho):\n",
    "    # 下向き正\n",
    "    return u_vert * -0.01, u_hori * 0.01, rho * 0.0001\n",
    "\n",
    "def target_transform(u_vert, u_hori, margin):\n",
    "    if margin == 0:\n",
    "        return u_vert * -0.01, u_hori * 0.01\n",
    "    else:\n",
    "        return u_vert[margin:-margin, margin:-margin] * -0.01, u_hori[margin:-margin, margin:-margin] * 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "True\n",
      "torch.Size([505, 481])\n"
     ]
    }
   ],
   "source": [
    "# test for WeatherDataset\n",
    "datetimeset = [dt.datetime(2011, 7, 1), dt.datetime(2011, 7, 2)]\n",
    "src_dir = '../lab_data/npy'\n",
    "weather_dataset = WeatherDataset(src_dir, datetimeset, transform, target_transform, 1)\n",
    "u_vert_direct = torch.from_numpy(np.load('../lab_data/npy/u_vert_2011070103.npy'))\n",
    "u_vert_target = weather_dataset.__getitem__(0)['u_vert_target']\n",
    "print(weather_dataset.__len__()) # 2\n",
    "print(torch.equal(u_vert_target, u_vert_direct[1:-1, 1:-1] * -0.01)) # True\n",
    "print(u_vert_direct.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a dataset and dataloader...\n",
      "Creating the model...\n",
      "\n",
      "bat_size =   1 \n",
      "optimizer = Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: False\n",
      "    lr: 0.1\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "max_epochs =  10 \n",
      "lrn_rate = 0.100 \n",
      "Training the model...\n",
      "batch: 0\n",
      "loss: 4366.02197265625\n",
      "batch: 50\n",
      "loss: 6154.4404296875\n",
      "batch: 100\n",
      "loss: 6478.8896484375\n",
      "batch: 150\n",
      "loss: 6058.3896484375\n",
      "batch: 200\n",
      "loss: 5819.2470703125\n",
      "batch: 250\n",
      "loss: 6781.2138671875\n",
      "batch: 300\n",
      "loss: 5816.2353515625\n",
      "batch: 350\n",
      "loss: 5709.02197265625\n",
      "batch: 400\n",
      "loss: 5237.18408203125\n",
      "batch: 450\n",
      "loss: 5901.3203125\n",
      "batch: 500\n",
      "loss: 6972.65771484375\n",
      "batch: 550\n",
      "loss: 6125.12451171875\n",
      "batch: 600\n",
      "loss: 5593.2314453125\n",
      "batch: 650\n",
      "loss: 5190.61328125\n",
      "batch: 700\n",
      "loss: 6838.43701171875\n",
      "epoch =    0   loss = 4560034.5342\n",
      "batch: 0\n",
      "loss: 5358.20263671875\n",
      "batch: 50\n",
      "loss: 4392.283203125\n",
      "batch: 100\n",
      "loss: 5157.0712890625\n",
      "batch: 150\n",
      "loss: 6508.419921875\n",
      "batch: 200\n",
      "loss: 6640.1669921875\n",
      "batch: 250\n",
      "loss: 5335.63330078125\n",
      "batch: 300\n",
      "loss: 6189.36865234375\n",
      "batch: 350\n",
      "loss: 5559.55419921875\n",
      "batch: 400\n",
      "loss: 7537.20751953125\n",
      "batch: 450\n",
      "loss: 5575.8349609375\n",
      "batch: 500\n",
      "loss: 6115.81298828125\n",
      "batch: 550\n",
      "loss: 4529.45654296875\n",
      "batch: 600\n",
      "loss: 5910.5439453125\n",
      "batch: 650\n",
      "loss: 7993.3251953125\n",
      "batch: 700\n",
      "loss: 6459.60888671875\n",
      "epoch =    1   loss = 4152131.5269\n",
      "batch: 0\n",
      "loss: 4886.94140625\n",
      "batch: 50\n",
      "loss: 5830.619140625\n",
      "batch: 100\n",
      "loss: 5153.02294921875\n",
      "batch: 150\n",
      "loss: 6359.439453125\n",
      "batch: 200\n",
      "loss: 4796.716796875\n",
      "batch: 250\n",
      "loss: 5857.40283203125\n",
      "batch: 300\n",
      "loss: 4928.90087890625\n",
      "batch: 350\n",
      "loss: 6839.78759765625\n",
      "batch: 400\n",
      "loss: 5221.94775390625\n",
      "batch: 450\n",
      "loss: 4637.80908203125\n",
      "batch: 500\n",
      "loss: 6009.61474609375\n",
      "batch: 550\n",
      "loss: 5550.396484375\n",
      "batch: 600\n",
      "loss: 6290.4736328125\n",
      "batch: 650\n",
      "loss: 6466.27587890625\n",
      "batch: 700\n",
      "loss: 5197.1982421875\n",
      "epoch =    2   loss = 4079484.3984\n",
      "batch: 0\n",
      "loss: 4855.3115234375\n",
      "batch: 50\n",
      "loss: 5778.763671875\n",
      "batch: 100\n",
      "loss: 5937.591796875\n",
      "batch: 150\n",
      "loss: 4256.31005859375\n",
      "batch: 200\n",
      "loss: 4900.38330078125\n",
      "batch: 250\n",
      "loss: 4461.77880859375\n",
      "batch: 300\n",
      "loss: 6145.69189453125\n",
      "batch: 350\n",
      "loss: 5178.43896484375\n",
      "batch: 400\n",
      "loss: 5144.8408203125\n",
      "batch: 450\n",
      "loss: 4171.6376953125\n",
      "batch: 500\n",
      "loss: 5034.63623046875\n",
      "batch: 550\n",
      "loss: 6879.69921875\n",
      "batch: 600\n",
      "loss: 6757.6640625\n",
      "batch: 650\n",
      "loss: 5525.28759765625\n",
      "batch: 700\n",
      "loss: 4249.51513671875\n",
      "epoch =    3   loss = 4038049.4656\n",
      "batch: 0\n",
      "loss: 4937.38671875\n",
      "batch: 50\n",
      "loss: 4998.1865234375\n",
      "batch: 100\n",
      "loss: 7030.2412109375\n",
      "batch: 150\n",
      "loss: 4747.546875\n",
      "batch: 200\n",
      "loss: 4986.9833984375\n",
      "batch: 250\n",
      "loss: 6478.8681640625\n",
      "batch: 300\n",
      "loss: 5350.626953125\n",
      "batch: 350\n",
      "loss: 5142.16357421875\n",
      "batch: 400\n",
      "loss: 5991.7353515625\n",
      "batch: 450\n",
      "loss: 4596.77685546875\n",
      "batch: 500\n",
      "loss: 6052.8544921875\n",
      "batch: 550\n",
      "loss: 6320.09716796875\n",
      "batch: 600\n",
      "loss: 4909.9248046875\n",
      "batch: 650\n",
      "loss: 4481.10888671875\n",
      "batch: 700\n",
      "loss: 7012.953125\n",
      "epoch =    4   loss = 4012103.5461\n",
      "batch: 0\n",
      "loss: 5708.7578125\n",
      "batch: 50\n",
      "loss: 4722.13671875\n",
      "batch: 100\n",
      "loss: 5397.88720703125\n",
      "batch: 150\n",
      "loss: 6309.0283203125\n",
      "batch: 200\n",
      "loss: 4946.16845703125\n",
      "batch: 250\n",
      "loss: 5651.17431640625\n",
      "batch: 300\n",
      "loss: 5159.4755859375\n",
      "batch: 350\n",
      "loss: 5006.125\n",
      "batch: 400\n",
      "loss: 5634.4658203125\n",
      "batch: 450\n",
      "loss: 6411.369140625\n",
      "batch: 500\n",
      "loss: 4575.33740234375\n",
      "batch: 550\n",
      "loss: 4741.99609375\n",
      "batch: 600\n",
      "loss: 5677.66162109375\n",
      "batch: 650\n",
      "loss: 5170.44970703125\n",
      "batch: 700\n",
      "loss: 4489.5732421875\n",
      "epoch =    5   loss = 3998957.2278\n",
      "batch: 0\n",
      "loss: 5063.1611328125\n",
      "batch: 50\n",
      "loss: 6258.83447265625\n",
      "batch: 100\n",
      "loss: 5664.8154296875\n",
      "batch: 150\n",
      "loss: 5251.47802734375\n",
      "batch: 200\n",
      "loss: 6081.337890625\n",
      "batch: 250\n",
      "loss: 5290.4345703125\n",
      "batch: 300\n",
      "loss: 4840.43212890625\n",
      "batch: 350\n",
      "loss: 5178.69677734375\n",
      "batch: 400\n",
      "loss: 4970.37109375\n",
      "batch: 450\n",
      "loss: 5957.30712890625\n",
      "batch: 500\n",
      "loss: 5979.5341796875\n",
      "batch: 550\n",
      "loss: 5132.35205078125\n",
      "batch: 600\n",
      "loss: 6581.29833984375\n",
      "batch: 650\n",
      "loss: 4901.16259765625\n",
      "batch: 700\n",
      "loss: 7731.974609375\n",
      "epoch =    6   loss = 3988578.0537\n",
      "batch: 0\n",
      "loss: 5986.14208984375\n",
      "batch: 50\n",
      "loss: 4690.35595703125\n",
      "batch: 100\n",
      "loss: 6258.1416015625\n",
      "batch: 150\n",
      "loss: 4834.11572265625\n",
      "batch: 200\n",
      "loss: 6073.18212890625\n",
      "batch: 250\n",
      "loss: 4751.302734375\n",
      "batch: 300\n",
      "loss: 7079.125\n",
      "batch: 350\n",
      "loss: 5727.0380859375\n",
      "batch: 400\n",
      "loss: 6047.57763671875\n",
      "batch: 450\n",
      "loss: 4721.1572265625\n",
      "batch: 500\n",
      "loss: 6171.87841796875\n",
      "batch: 550\n",
      "loss: 4438.0791015625\n",
      "batch: 600\n",
      "loss: 5672.0283203125\n",
      "batch: 650\n",
      "loss: 6164.560546875\n",
      "batch: 700\n",
      "loss: 4691.345703125\n",
      "epoch =    7   loss = 3973797.3557\n",
      "batch: 0\n",
      "loss: 4937.333984375\n",
      "batch: 50\n",
      "loss: 4996.11376953125\n",
      "batch: 100\n",
      "loss: 5211.259765625\n",
      "batch: 150\n",
      "loss: 4727.52392578125\n",
      "batch: 200\n",
      "loss: 4872.03271484375\n",
      "batch: 250\n",
      "loss: 5841.248046875\n",
      "batch: 300\n",
      "loss: 5717.4873046875\n",
      "batch: 350\n",
      "loss: 5184.2724609375\n",
      "batch: 400\n",
      "loss: 6173.75927734375\n",
      "batch: 450\n",
      "loss: 6000.23876953125\n",
      "batch: 500\n",
      "loss: 5400.4072265625\n",
      "batch: 550\n",
      "loss: 5318.52880859375\n",
      "batch: 600\n",
      "loss: 4877.39501953125\n",
      "batch: 650\n",
      "loss: 7154.53369140625\n",
      "batch: 700\n",
      "loss: 6341.25244140625\n",
      "epoch =    8   loss = 3966760.1279\n",
      "batch: 0\n",
      "loss: 6026.54248046875\n",
      "batch: 50\n",
      "loss: 5029.53564453125\n",
      "batch: 100\n",
      "loss: 4450.4765625\n",
      "batch: 150\n",
      "loss: 4762.94873046875\n",
      "batch: 200\n",
      "loss: 4711.84033203125\n",
      "batch: 250\n",
      "loss: 4778.44091796875\n",
      "batch: 300\n",
      "loss: 5921.29541015625\n",
      "batch: 350\n",
      "loss: 4467.736328125\n",
      "batch: 400\n",
      "loss: 5110.66943359375\n",
      "batch: 450\n",
      "loss: 4549.24658203125\n",
      "batch: 500\n",
      "loss: 5317.1640625\n",
      "batch: 550\n",
      "loss: 6069.935546875\n",
      "batch: 600\n",
      "loss: 5819.53271484375\n",
      "batch: 650\n",
      "loss: 6070.375\n",
      "batch: 700\n",
      "loss: 5409.92236328125\n",
      "epoch =    9   loss = 3952690.7280\n",
      "Done\n",
      "Computing the model accuracy\n"
     ]
    }
   ],
   "source": [
    "# todo: enable the model to receive batch data\n",
    "def main():\n",
    "    print('Creating a dataset and dataloader...')\n",
    "    data_dir = '../lab_data/npy'\n",
    "    datetimeset = [dt.datetime(2011+i, 7, 1, 0) + timedelta(days=j) for i in range(10) for j in range(92)]\n",
    "    dataset = WeatherDataset(data_dir, datetimeset, transform, target_transform, LBM.margin)\n",
    "    batch_size = 1\n",
    "    train_split = 0.8\n",
    "    shuffle_dataset = True # if false, this model will learn only by the 'past' data, and forcast the 'future' data.\n",
    "    random_seed = 42\n",
    "    \n",
    "    dataset_size =  len(dataset)\n",
    "    indices = list(range(dataset_size))\n",
    "    split = int(np.floor(train_split * dataset_size))\n",
    "    if shuffle_dataset:\n",
    "        np.random.seed(random_seed)\n",
    "        np.random.shuffle(indices)\n",
    "    train_indices, valid_indices = indices[:split], indices[split:]\n",
    "    \n",
    "    train_sampler = SubsetRandomSampler(train_indices)\n",
    "    valid_sampler = SubsetRandomSampler(valid_indices)\n",
    "    \n",
    "    train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler, num_workers=4)\n",
    "    valid_sampler = DataLoader(dataset, batch_size=batch_size, sampler=valid_sampler, num_workers=4)\n",
    "    \n",
    "    print('Creating the model...')\n",
    "    lbm = LBM(505, 481).to(device) # got by the prev cell\n",
    "    \n",
    "    num_epochs = 10\n",
    "    learning_rate = 0.1\n",
    "    \n",
    "    optimizer = torch.optim.Adam(lbm.parameters(), lr=learning_rate)\n",
    "    \n",
    "    print(\"\\nbat_size = %3d \" % batch_size)\n",
    "    print(\"optimizer = \" + str(optimizer))\n",
    "    print(\"max_epochs = %3d \" % num_epochs)\n",
    "    print(\"lrn_rate = %0.3f \" % learning_rate)\n",
    "    \n",
    "    print('Training the model...')\n",
    "    lbm.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for (batch_idx, batch) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            u_vert_out, u_hori_out = lbm(batch['u_vert'].squeeze(), batch['u_hori'].squeeze(), batch['rho'].squeeze())\n",
    "            loss_val = loss_func(u_vert_out, u_hori_out, batch['u_vert_target'].squeeze(), batch['u_hori_target'].squeeze())\n",
    "            epoch_loss += loss_val.item()\n",
    "            loss_val.backward()\n",
    "            if batch_idx % 50 == 0:\n",
    "                print(f'batch: {batch_idx}')\n",
    "                print(f'loss: {loss_val.item()}')\n",
    "            optimizer.step()\n",
    "            \n",
    "        print(\"epoch = %4d   loss = %0.4f\" % (epoch, epoch_loss))\n",
    "    print('Done')\n",
    "    \n",
    "    print('Computing the model accuracy')\n",
    "    lbm.eval()\n",
    "        \n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8cc852ad98fd153e44a223758c85fc1b400de02ae4f1d397854b0c01b67255b7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
