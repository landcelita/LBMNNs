{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/landcelita/LBMNNs/blob/master/LBMNNs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNmformq_4SL"
      },
      "source": [
        "# Prepare"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iltDTo-5uQCo",
        "outputId": "85331410-ec69-46d8-b1a3-806c3576eead"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pygrib in /usr/local/lib/python3.8/dist-packages (2.1.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from pygrib) (1.21.6)\n",
            "Requirement already satisfied: pyproj in /usr/local/lib/python3.8/dist-packages (from pygrib) (3.4.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.8/dist-packages (from pyproj->pygrib) (2022.12.7)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorboardX in /usr/local/lib/python3.8/dist-packages (2.5.1)\n",
            "Requirement already satisfied: protobuf<=3.20.1,>=3.8.0 in /usr/local/lib/python3.8/dist-packages (from tensorboardX) (3.19.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from tensorboardX) (1.21.6)\n"
          ]
        }
      ],
      "source": [
        "!pip install pygrib\n",
        "!pip install tensorboardX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Msj8FXQPMa4U"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dz9u0saErhps"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import os\n",
        "import time\n",
        "import datetime as dt\n",
        "import pygrib\n",
        "from datetime import timedelta\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import torch.multiprocessing as multiprocessing\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from typing import Dict, List"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EHaRtnCTrsIb",
        "outputId": "5a1a98cd-6720-4637-a091-e70045b41313"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPOnQVMmlzea"
      },
      "source": [
        "# Const"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QJTc-xtQlQ7G"
      },
      "outputs": [],
      "source": [
        "U_COEF = 0.0025\n",
        "RHO_COEF = 0.0001\n",
        "U_MSE_COEF = 160000.\n",
        "RHO_MSE_COEF = 200.\n",
        "RANDOM_WIDTH = 0.01\n",
        "HEIGHT = 505\n",
        "WIDTH = 481"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGT_tdWuATR5"
      },
      "source": [
        "# Define custom layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrSS-r1xDGm9"
      },
      "source": [
        "### Input Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cuOiERZ-r6oB"
      },
      "outputs": [],
      "source": [
        "class InputLayer(nn.Module):\n",
        "    def __init__(self, height, width):\n",
        "        super().__init__()\n",
        "        self.height = height\n",
        "        self.width = width\n",
        "        self.c_in = torch.zeros((3, 3, self.height, self.width), dtype=torch.float32, device=device)\n",
        "        for dr in range(-1, 2):\n",
        "            for dc in range(-1, 2):\n",
        "                if abs(dr) + abs(dc) == 0:\n",
        "                    self.c_in[dr+1, dc+1, :, :] = 4.0/9.0\n",
        "                elif abs(dr) + abs(dc) == 1:\n",
        "                    self.c_in[dr+1, dc+1, :, :] = 1.0/9.0\n",
        "                elif abs(dr) + abs(dc) == 2:\n",
        "                    self.c_in[dr+1, dc+1, :, :] = 1.0/36.0\n",
        "        self.c = nn.Parameter(self.c_in, requires_grad=True)\n",
        "        \n",
        "    def forward(self, u_vert, u_hori, rho):\n",
        "        # use feq as input field\n",
        "        # todo: make this layer be also learnable\n",
        "        \n",
        "        # feq(x,v) = C[v] * rho * (1 + 3 v.u + 4.5 (v.u)^2 - 1.5 u^2)\n",
        "        feq = torch.empty((3, 3, self.height, self.width), dtype=torch.float32, device=device)\n",
        "        u_squared = u_vert ** 2 + u_hori ** 2\n",
        "        \n",
        "        for dr in range(-1, 2):\n",
        "            for dc in range(-1, 2):\n",
        "                v_dot_u = dr * u_vert + dc * u_hori\n",
        "                feq[dr+1, dc+1, :, :] = self.c[dr+1,dc+1,:,:] * rho * (1.0 + 3.0 * v_dot_u + 4.5 * v_dot_u * v_dot_u - 1.5 * u_squared)\n",
        "                \n",
        "        return feq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cymZiWDGr8bd",
        "outputId": "3270dcfc-104d-4bba-a2ac-fb145688447d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(0.1000, device='cuda:0')\n",
            "tensor(-0.3000, device='cuda:0')\n",
            "tensor(0.8000, device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "# test for InputLayer\n",
        "# # Later, I'll check the immutability of the InputLayer\n",
        "#  - it must put out the same tensor even after the weights are updated\n",
        "def test_for_InputLayer():\n",
        "    input_layer_u_vert = torch.tensor([[0.1]], dtype=torch.float32, device=device)\n",
        "    input_layer_u_hori = torch.tensor([[-0.3]], dtype=torch.float32, device=device)\n",
        "    input_layer_rho = torch.tensor([[0.8]], dtype=torch.float32, device=device)\n",
        "\n",
        "    input_layer = InputLayer(1, 1).to(device)\n",
        "    feq = input_layer(input_layer_u_vert, input_layer_u_hori, input_layer_rho)\n",
        "    print(torch.sum(feq[2,:,0,0] - feq[0,:,0,0]) / torch.sum(feq[:,:,0,0])) # u_vert\n",
        "    print(torch.sum(feq[:,2,0,0] - feq[:,0,0,0]) / torch.sum(feq[:,:,0,0])) # u_hori\n",
        "    print(torch.sum(feq[:,:,0,0])) # rho\n",
        "\n",
        "test_for_InputLayer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3htDZ5qVDOGh"
      },
      "source": [
        "### Streaming Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j_xf25cwsBy7"
      },
      "outputs": [],
      "source": [
        "class StreamingLayer(nn.Module):\n",
        "    def __init__(self, in_height, in_width):\n",
        "        super().__init__()\n",
        "        self.in_height = in_height\n",
        "        self.in_width = in_width\n",
        "        \n",
        "        self.w0 = nn.Parameter(torch.zeros((in_height-2, in_width-2), dtype=torch.float32, device=device))\n",
        "        self.w1 = nn.Parameter(torch.ones((in_height-2, in_width-2), dtype=torch.float32, device=device))\n",
        "        \n",
        "    def forward(self, f_prev):\n",
        "        # f(x,v) = w0(x,v) + w1(x,v) * f_prev(x-v,v)\n",
        "        \n",
        "        f = torch.empty((3, 3, self.in_height-2, self.in_width-2), dtype=torch.float32, device=device)\n",
        "        for dr in range(-1, 2):\n",
        "            for dc in range(-1, 2):\n",
        "                f[dr+1,dc+1,:,:] = self.w0 + self.w1 * f_prev[dr+1,dc+1,1-dr:self.in_height-1-dr,1-dc:self.in_width-1-dc]\n",
        "        \n",
        "        return f\n",
        "                "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oxhpr5208GNM"
      },
      "outputs": [],
      "source": [
        "class StreamingLayerWithNoisyInit(nn.Module):\n",
        "    def __init__(self, in_height, in_width):\n",
        "        super().__init__()\n",
        "        self.in_height = in_height\n",
        "        self.in_width = in_width\n",
        "        \n",
        "        self.w0 = nn.Parameter(torch.zeros((in_height-2, in_width-2), dtype=torch.float32, device=device)\\\n",
        "                               + torch.rand((in_height-2, in_width-2), dtype=torch.float32, device=device) * RANDOM_WIDTH - RANDOM_WIDTH/2.0)\n",
        "        self.w1 = nn.Parameter(torch.ones((in_height-2, in_width-2), dtype=torch.float32, device=device)\\\n",
        "                               + torch.rand((in_height-2, in_width-2), dtype=torch.float32, device=device) * RANDOM_WIDTH - RANDOM_WIDTH/2.0)\n",
        "        \n",
        "    def forward(self, f_prev):\n",
        "        # f(x,v) = w0(x,v) + w1(x,v) * f_prev(x-v,v)\n",
        "        \n",
        "        f = torch.empty((3, 3, self.in_height-2, self.in_width-2), dtype=torch.float32, device=device)\n",
        "        for dr in range(-1, 2):\n",
        "            for dc in range(-1, 2):\n",
        "                f[dr+1,dc+1,:,:] = self.w0 + self.w1 * f_prev[dr+1,dc+1,1-dr:self.in_height-1-dr,1-dc:self.in_width-1-dc]\n",
        "        \n",
        "        return f"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fw9Mo_MOsFCP",
        "outputId": "565474a6-a750-495b-c5d0-3d2abb620620"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[[0., 0., 0.],\n",
            "          [0., 0., 0.],\n",
            "          [0., 0., 9.]],\n",
            "\n",
            "         [[0., 0., 0.],\n",
            "          [0., 0., 0.],\n",
            "          [0., 8., 0.]],\n",
            "\n",
            "         [[0., 0., 0.],\n",
            "          [0., 0., 0.],\n",
            "          [7., 0., 0.]]],\n",
            "\n",
            "\n",
            "        [[[0., 0., 0.],\n",
            "          [0., 0., 6.],\n",
            "          [0., 0., 0.]],\n",
            "\n",
            "         [[0., 0., 0.],\n",
            "          [0., 5., 0.],\n",
            "          [0., 0., 0.]],\n",
            "\n",
            "         [[0., 0., 0.],\n",
            "          [4., 0., 0.],\n",
            "          [0., 0., 0.]]],\n",
            "\n",
            "\n",
            "        [[[0., 0., 3.],\n",
            "          [0., 0., 0.],\n",
            "          [0., 0., 0.]],\n",
            "\n",
            "         [[0., 2., 0.],\n",
            "          [0., 0., 0.],\n",
            "          [0., 0., 0.]],\n",
            "\n",
            "         [[1., 0., 0.],\n",
            "          [0., 0., 0.],\n",
            "          [0., 0., 0.]]]], device='cuda:0', dtype=torch.float64)\n",
            "tensor([[[[9.]],\n",
            "\n",
            "         [[8.]],\n",
            "\n",
            "         [[7.]]],\n",
            "\n",
            "\n",
            "        [[[6.]],\n",
            "\n",
            "         [[5.]],\n",
            "\n",
            "         [[4.]]],\n",
            "\n",
            "\n",
            "        [[[3.]],\n",
            "\n",
            "         [[2.]],\n",
            "\n",
            "         [[1.]]]], device='cuda:0', grad_fn=<CopySlices>)\n"
          ]
        }
      ],
      "source": [
        "# test for StreamingLayer\n",
        "def test_for_StreamingLayer():\n",
        "    input_f_prev = torch.zeros((3, 3, 3, 3), dtype=torch.float64, device=device)\n",
        "    input_f_prev[2,2,0,0] = 1.0\n",
        "    input_f_prev[2,1,0,1] = 2.0\n",
        "    input_f_prev[2,0,0,2] = 3.0\n",
        "    input_f_prev[1,2,1,0] = 4.0\n",
        "    input_f_prev[1,1,1,1] = 5.0\n",
        "    input_f_prev[1,0,1,2] = 6.0\n",
        "    input_f_prev[0,2,2,0] = 7.0\n",
        "    input_f_prev[0,1,2,1] = 8.0\n",
        "    input_f_prev[0,0,2,2] = 9.0\n",
        "    print(input_f_prev)\n",
        "\n",
        "    streaming_layer = StreamingLayer(3, 3).to(device)\n",
        "    f_streamed = streaming_layer(input_f_prev)\n",
        "    print(f_streamed)\n",
        "\n",
        "test_for_StreamingLayer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8F4aZ1WDlEc"
      },
      "source": [
        "### Colliding Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p1qYEjs3sH_1"
      },
      "outputs": [],
      "source": [
        "class CollidingLayer(nn.Module):\n",
        "    def __init__(self, in_height, in_width):\n",
        "        super().__init__()\n",
        "        self.in_height = in_height\n",
        "        self.in_width = in_width\n",
        "        \n",
        "        self.w1 = nn.Parameter(torch.full((in_height, in_width), fill_value=3.0, dtype=torch.float32, device=device), requires_grad=True)\n",
        "        self.w2 = nn.Parameter(torch.full((in_height, in_width), fill_value=0.0, dtype=torch.float32, device=device), requires_grad=True)\n",
        "        self.w3 = nn.Parameter(torch.full((in_height, in_width), fill_value=4.5, dtype=torch.float32, device=device), requires_grad=True)\n",
        "        self.w4 = nn.Parameter(torch.full((in_height, in_width), fill_value=-1.5, dtype=torch.float32, device=device), requires_grad=True)\n",
        "        \n",
        "    def forward(self, f_prev):\n",
        "        # feq(x,v) = C(v) * rho(x) * (1 + w1(x,v) v.u(x) + w2(x,v) vXu(x) + w3(x,v) (v.u(x))^2 + w4(x,v) u(x)^2)\n",
        "        # f = (f_prev + feq) / 2\n",
        "        C = torch.tensor([[1.0/36.0, 1.0/9.0, 1.0/36.0], [1.0/9.0, 4.0/9.0, 1.0/9.0], [1.0/36.0, 1.0/9.0, 1.0/36.0]], dtype=torch.float32, device=device)\n",
        "        \n",
        "        rho = torch.sum(f_prev, dim=(0,1))\n",
        "        u_vert = (f_prev[2,0,:,:] + f_prev[2,1,:,:] + f_prev[2,2,:,:] - f_prev[0,0,:,:] - f_prev[0,1,:,:] - f_prev[0,2,:,:]) / rho\n",
        "        u_hori = (f_prev[0,2,:,:] + f_prev[1,2,:,:] + f_prev[2,2,:,:] - f_prev[0,0,:,:] - f_prev[1,0,:,:] - f_prev[2,0,:,:]) / rho\n",
        "        u_squared = u_vert ** 2 + u_hori ** 2\n",
        "        feq = torch.empty((3, 3, self.in_height, self.in_width), dtype=torch.float32, device=device)\n",
        "        \n",
        "        for dr in range(-1, 2):\n",
        "            for dc in range(-1, 2):\n",
        "                v_dot_u = dr * u_vert + dc * u_hori\n",
        "                vXu = dr * u_hori - dc * u_vert\n",
        "                feq[dr+1, dc+1, :, :] = C[dr+1,dc+1] * rho * (1.0 + (self.w1 + self.w3 * v_dot_u) * v_dot_u + self.w2 * vXu + self.w4 * u_squared)\n",
        "        \n",
        "        return (f_prev + feq) / 2.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c1p9lfUt5qpI"
      },
      "outputs": [],
      "source": [
        "class CollidingLayerWithNoisyInit(nn.Module):\n",
        "    def __init__(self, in_height, in_width):\n",
        "        super().__init__()\n",
        "        self.in_height = in_height\n",
        "        self.in_width = in_width\n",
        "        \n",
        "        self.w1 = nn.Parameter(torch.full((in_height, in_width), fill_value=3.0, dtype=torch.float32, device=device)\\\n",
        "                               + torch.rand((in_height, in_width), dtype=torch.float32, device=device) * RANDOM_WIDTH - RANDOM_WIDTH/2.0, requires_grad=True)\n",
        "        self.w2 = nn.Parameter(torch.full((in_height, in_width), fill_value=0.0, dtype=torch.float32, device=device)\\\n",
        "                               + torch.rand((in_height, in_width), dtype=torch.float32, device=device) * RANDOM_WIDTH - RANDOM_WIDTH/2.0, requires_grad=True)\n",
        "        self.w3 = nn.Parameter(torch.full((in_height, in_width), fill_value=4.5, dtype=torch.float32, device=device)\\\n",
        "                               + torch.rand((in_height, in_width), dtype=torch.float32, device=device) * RANDOM_WIDTH - RANDOM_WIDTH/2.0, requires_grad=True)\n",
        "        self.w4 = nn.Parameter(torch.full((in_height, in_width), fill_value=-1.5, dtype=torch.float32, device=device)\\\n",
        "                               + torch.rand((in_height, in_width), dtype=torch.float32, device=device) * RANDOM_WIDTH - RANDOM_WIDTH/2.0, requires_grad=True)\n",
        "        self.w5 = nn.Parameter(torch.full((in_height, in_width), fill_value=1.0, dtype=torch.float32, device=device)\\\n",
        "                               + torch.rand((in_height, in_width), dtype=torch.float32, device=device) * RANDOM_WIDTH - RANDOM_WIDTH/2.0, requires_grad=True)\n",
        "        \n",
        "    def forward(self, f_prev):\n",
        "        # feq(x,v) = C(v) * rho(x) * (1 + w1(x,v) v.u(x) + w2(x,v) vXu(x) + w3(x,v) (v.u(x))^2 + w4(x,v) u(x)^2)\n",
        "        # f = (f_prev + feq) / 2\n",
        "        C = torch.tensor([[1.0/36.0, 1.0/9.0, 1.0/36.0], [1.0/9.0, 4.0/9.0, 1.0/9.0], [1.0/36.0, 1.0/9.0, 1.0/36.0]], dtype=torch.float32, device=device)\n",
        "        \n",
        "        rho = torch.sum(f_prev, dim=(0,1))\n",
        "        u_vert = (f_prev[2,0,:,:] + f_prev[2,1,:,:] + f_prev[2,2,:,:] - f_prev[0,0,:,:] - f_prev[0,1,:,:] - f_prev[0,2,:,:]) / rho\n",
        "        u_hori = (f_prev[0,2,:,:] + f_prev[1,2,:,:] + f_prev[2,2,:,:] - f_prev[0,0,:,:] - f_prev[1,0,:,:] - f_prev[2,0,:,:]) / rho\n",
        "        u_squared = u_vert ** 2 + u_hori ** 2\n",
        "        feq = torch.empty((3, 3, self.in_height, self.in_width), dtype=torch.float32, device=device)\n",
        "        \n",
        "        for dr in range(-1, 2):\n",
        "            for dc in range(-1, 2):\n",
        "                v_dot_u = dr * u_vert + dc * u_hori\n",
        "                vXu = dr * u_hori - dc * u_vert\n",
        "                feq[dr+1, dc+1, :, :] = C[dr+1,dc+1] * rho * (1.0 + (self.w1 + self.w3 * v_dot_u) * v_dot_u + self.w2 * vXu + self.w4 * u_squared)\n",
        "        \n",
        "        return (f_prev * self.w5 + feq * (2.0 - self.w5)) / 2.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cAzNVUB0sMSw",
        "outputId": "a6927209-fa37-46a5-cd71-9b5894e53c1f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(45., device='cuda:0', grad_fn=<SumBackward0>)\n",
            "tensor(-0.4000, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "-0.4\n",
            "tensor(-0.1333, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "-0.13333333333333333\n"
          ]
        }
      ],
      "source": [
        "# test for CollidingLayer\n",
        "def test_for_CollidingLayer():\n",
        "    input_f = torch.tensor([[[[9., 9.]],[[8., 8.]],[[7., 7.]]],[[[6., 6.]],[[5., 5.]],[[4., 4.]]],[[[3., 3.]],[[2., 2.]],[[1., 1.]]]], dtype=torch.float32, device=device)\n",
        "    colliding_layer = CollidingLayer(1, 2)\n",
        "    collided_field = colliding_layer(input_f)\n",
        "\n",
        "    print(torch.sum(collided_field[:,:,0,0])) # rho 45.0\n",
        "    print((torch.sum(collided_field[2,:,0,0]) - torch.sum(collided_field[0,:,0,0])) / torch.sum(collided_field[:,:,0,0])) # u_vert\n",
        "    print((1.+2.+3.-9.-8.-7.)/45.)\n",
        "    print((torch.sum(collided_field[:,2,0,0]) - torch.sum(collided_field[:,0,0,0])) / torch.sum(collided_field[:,:,0,0])) # u_hori\n",
        "    print((1.+4.+7.-3.-6.-9.)/45.)\n",
        "\n",
        "test_for_CollidingLayer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUXXov9xDqdJ"
      },
      "source": [
        "### Output Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OD2zGC-usOZm"
      },
      "outputs": [],
      "source": [
        "class OutputLayer(nn.Module):\n",
        "    def __init__(self, height, width):\n",
        "        super().__init__()\n",
        "        self.height = height\n",
        "        self.width = width\n",
        "        \n",
        "    def forward(self, f):\n",
        "        rho = torch.sum(f, dim=(0,1))\n",
        "        u_vert = (f[2,0,:,:] + f[2,1,:,:] + f[2,2,:,:] - f[0,0,:,:] - f[0,1,:,:] - f[0,2,:,:]) / rho\n",
        "        u_hori = (f[0,2,:,:] + f[1,2,:,:] + f[2,2,:,:] - f[0,0,:,:] - f[1,0,:,:] - f[2,0,:,:]) / rho\n",
        "        return {\n",
        "            'u_vert': u_vert,\n",
        "            'u_hori': u_hori,\n",
        "            'rho': rho\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGh2mZnDAvjQ"
      },
      "source": [
        "# Construct a LBM model from the layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uJllOre8sQ7Q"
      },
      "outputs": [],
      "source": [
        "class LBM(nn.Module):\n",
        "    margin = 4\n",
        "    \n",
        "    def __init__(self, height, width):\n",
        "        super(LBM, self).__init__()\n",
        "        self.input_layer = InputLayer(height, width)\n",
        "        self.streaming1_layer = StreamingLayer(height, width)\n",
        "        self.colliding1_layer = CollidingLayer(height-2, width-2)\n",
        "        self.streaming2_layer = StreamingLayer(height-2, width-2)\n",
        "        self.colliding2_layer = CollidingLayer(height-4, width-4)\n",
        "        self.streaming3_layer = StreamingLayer(height-4, width-4)\n",
        "        self.colliding3_layer = CollidingLayer(height-6, width-6)\n",
        "        self.streaming4_layer = StreamingLayer(height-6, width-6)\n",
        "        self.output_layer = OutputLayer(height-8, width-8)\n",
        "        \n",
        "    def forward(self, u_vert, u_hori, rho):\n",
        "        f = self.input_layer(u_vert, u_hori, rho)\n",
        "        f = self.streaming1_layer(f)\n",
        "        f = self.colliding1_layer(f)\n",
        "        f = self.streaming2_layer(f)\n",
        "        f = self.colliding2_layer(f)\n",
        "        f = self.streaming3_layer(f)\n",
        "        f = self.colliding3_layer(f)\n",
        "        f = self.streaming4_layer(f)\n",
        "        out = self.output_layer(f)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fvk1yd5S9Ksm"
      },
      "outputs": [],
      "source": [
        "class LBMwithNoisyInit(nn.Module):\n",
        "    margin = 5\n",
        "    \n",
        "    def __init__(self, height, width):\n",
        "        super(LBMwithNoisyInit, self).__init__()\n",
        "        self.input_layer = InputLayer(height, width)\n",
        "        self.streaming1_layer = StreamingLayerWithNoisyInit(height, width)\n",
        "        self.colliding1_layer = CollidingLayerWithNoisyInit(height-2, width-2)\n",
        "        self.streaming2_layer = StreamingLayerWithNoisyInit(height-2, width-2)\n",
        "        self.colliding2_layer = CollidingLayerWithNoisyInit(height-4, width-4)\n",
        "        self.streaming3_layer = StreamingLayerWithNoisyInit(height-4, width-4)\n",
        "        self.colliding3_layer = CollidingLayerWithNoisyInit(height-6, width-6)\n",
        "        self.streaming4_layer = StreamingLayerWithNoisyInit(height-6, width-6)\n",
        "        self.colliding4_layer = CollidingLayerWithNoisyInit(height-8, width-8)\n",
        "        self.streaming5_layer = StreamingLayerWithNoisyInit(height-8, width-8)\n",
        "        self.output_layer = OutputLayer(height-10, width-10)\n",
        "        \n",
        "    def forward(self, u_vert, u_hori, rho):\n",
        "        f = self.input_layer(u_vert, u_hori, rho)\n",
        "        f = self.streaming1_layer(f)\n",
        "        f = self.colliding1_layer(f)\n",
        "        f = self.streaming2_layer(f)\n",
        "        f = self.colliding2_layer(f)\n",
        "        f = self.streaming3_layer(f)\n",
        "        f = self.colliding3_layer(f)\n",
        "        f = self.streaming4_layer(f)\n",
        "        f = self.colliding4_layer(f)\n",
        "        f = self.streaming5_layer(f)\n",
        "        out = self.output_layer(f)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_tbqMBgBlt6"
      },
      "source": [
        "# Load data to make a dataset and dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOWJ6-RxElK_"
      },
      "source": [
        "### Transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R4PkTjNMscJ9"
      },
      "outputs": [],
      "source": [
        "def transform(u_vert, u_hori, rho):\n",
        "    # 下向き正\n",
        "    return u_vert * -U_COEF, u_hori * U_COEF, rho * RHO_COEF\n",
        "\n",
        "def target_transform(u_vert, u_hori, rho, margin):\n",
        "    if margin == 0:\n",
        "        return u_vert * -U_COEF, u_hori * U_COEF, rho * RHO_COEF\n",
        "    else:\n",
        "        return u_vert[margin:-margin, margin:-margin] * -U_COEF, u_hori[margin:-margin, margin:-margin] * U_COEF, rho[margin:-margin, margin:-margin] * RHO_COEF\n",
        "\n",
        "def remove_margin(u_vert, u_hori, rho, margin):\n",
        "    if margin == 0:\n",
        "        return u_vert, u_hori, rho\n",
        "    else:\n",
        "        return u_vert[margin:-margin, margin:-margin], u_hori[margin:-margin, margin:-margin], rho[margin:-margin,margin:-margin]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oq6cB_5CEovi"
      },
      "source": [
        "### Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6te31vRosV-1"
      },
      "outputs": [],
      "source": [
        "class WeatherDatasetLazy(torch.utils.data.Dataset):\n",
        "    def __init__(self, src_dir, datetimes, transform, target_transform, margin):\n",
        "        self.src_dir = src_dir\n",
        "        self.input_paths = []\n",
        "        self.target_paths = []\n",
        "        self.datetimes = []\n",
        "        self.margin = margin # this value is the number of streaming layers that reduce the border cells.\n",
        "        # eg. ...                            xxx\n",
        "        #     ...  --(1 Streaming Layer)-->  x.x\n",
        "        #     ...                            xxx\n",
        "        for datetime in datetimes:\n",
        "            self.datetimes.append(datetime.strftime('%Y%m%d%H'))\n",
        "            self.input_paths.append(os.path.join(src_dir, datetime.strftime('%Y%m%d%H.grib2')))\n",
        "            datetime += timedelta(hours=3)\n",
        "            self.target_paths.append(os.path.join(src_dir, datetime.strftime('%Y%m%d%H.grib2')))\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.input_paths)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        datetime = self.datetimes[idx]\n",
        "        input_grib = pygrib.open(self.input_paths[idx])\n",
        "        rho = torch.from_numpy(np.array(input_grib.select()[0].values, dtype=np.float32)).to(device)\n",
        "        u_hori = torch.from_numpy(np.array(input_grib.select()[1].values, dtype=np.float32)).to(device)\n",
        "        u_vert = torch.from_numpy(np.array(input_grib.select()[2].values, dtype=np.float32)).to(device)\n",
        "        target_grib = pygrib.open(self.target_paths[idx])\n",
        "        rho_target = torch.from_numpy(np.array(target_grib.select()[0].values, dtype=np.float32)).to(device)\n",
        "        u_hori_target = torch.from_numpy(np.array(target_grib.select()[1].values, dtype=np.float32)).to(device)\n",
        "        u_vert_target = torch.from_numpy(np.array(target_grib.select()[2].values, dtype=np.float32)).to(device)\n",
        "        u_vert, u_hori, rho = self.transform(u_vert, u_hori, rho)\n",
        "        u_vert_target, u_hori_target, rho_target = self.target_transform(u_vert_target, u_hori_target, rho_target, self.margin)\n",
        "        return {\n",
        "            \"u_vert\": u_vert,\n",
        "            \"u_hori\": u_hori,\n",
        "            \"rho\": rho,\n",
        "            \"u_vert_target\": u_vert_target,\n",
        "            \"u_hori_target\": u_hori_target,\n",
        "            \"rho_target\": rho_target,\n",
        "            \"datetime\": datetime,\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nhjZS9lW8Ml9"
      },
      "outputs": [],
      "source": [
        "class WeatherDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, src_dir, datetimes, transform, target_transform, margin):\n",
        "        self.src_dir = src_dir\n",
        "        self.input_paths = []\n",
        "        self.target_paths = []\n",
        "        self.datetimes = []\n",
        "        self.margin = margin # this value is the number of streaming layers that reduce the border cells.\n",
        "        # eg. ...                            xxx\n",
        "        #     ...  --(1 Streaming Layer)-->  x.x\n",
        "        #     ...                            xxx\n",
        "        for datetime in datetimes:\n",
        "            self.datetimes.append(datetime.strftime('%Y%m%d%H'))\n",
        "            self.input_paths.append(os.path.join(src_dir, datetime.strftime('%Y%m%d%H.grib2')))\n",
        "            datetime += timedelta(hours=3)\n",
        "            self.target_paths.append(os.path.join(src_dir, datetime.strftime('%Y%m%d%H.grib2')))\n",
        "        self.rhos = []\n",
        "        self.u_horis = []\n",
        "        self.u_verts = []\n",
        "        self.rho_targets = []\n",
        "        self.u_hori_targets = []\n",
        "        self.u_vert_targets = []\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "        now = time.time()\n",
        "        for idx in range(len(self.input_paths)):\n",
        "            if (idx+1) % 100 == 0:\n",
        "                print(f'{idx+1} samples collected, {time.time() - now:.2f} s')\n",
        "                now = time.time()\n",
        "            input_grib = pygrib.open(self.input_paths[idx])\n",
        "            rho = torch.from_numpy(np.array(input_grib.select()[0].values, dtype=np.float32)).to(device)\n",
        "            u_hori = torch.from_numpy(np.array(input_grib.select()[1].values, dtype=np.float32)).to(device)\n",
        "            u_vert = torch.from_numpy(np.array(input_grib.select()[2].values, dtype=np.float32)).to(device)\n",
        "            target_grib = pygrib.open(self.target_paths[idx])\n",
        "            rho_target = torch.from_numpy(np.array(target_grib.select()[0].values, dtype=np.float32)).to(device)\n",
        "            u_hori_target = torch.from_numpy(np.array(target_grib.select()[1].values, dtype=np.float32)).to(device)\n",
        "            u_vert_target = torch.from_numpy(np.array(target_grib.select()[2].values, dtype=np.float32)).to(device)\n",
        "            self.rhos.append(rho)\n",
        "            self.u_horis.append(u_hori)\n",
        "            self.u_verts.append(u_vert)\n",
        "            self.rho_targets.append(rho_target)\n",
        "            self.u_hori_targets.append(u_hori_target)\n",
        "            self.u_vert_targets.append(u_vert_target)\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.input_paths)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        datetime = self.datetimes[idx]\n",
        "        u_vert, u_hori, rho = self.transform(self.u_verts[idx], self.u_horis[idx], self.rhos[idx])\n",
        "        u_vert_target, u_hori_target, rho_target = self.target_transform(self.u_vert_targets[idx], self.u_hori_targets[idx], self.rho_targets[idx], self.margin)\n",
        "        return {\n",
        "            \"u_vert\": u_vert,\n",
        "            \"u_hori\": u_hori,\n",
        "            \"rho\": rho,\n",
        "            \"u_vert_target\": u_vert_target,\n",
        "            \"u_hori_target\": u_hori_target,\n",
        "            \"rho_target\": rho_target,\n",
        "            \"datetime\": datetime\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w0TlxIcKsev_",
        "outputId": "4463bc42-cf25-415b-d02e-64f7b128cdec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2\n",
            "False\n",
            "torch.Size([505, 481])\n"
          ]
        }
      ],
      "source": [
        "# test for WeatherDataset\n",
        "def test_for_weatherdataset():\n",
        "    datetimeset = [dt.datetime(2011, 7, 1), dt.datetime(2011, 7, 2)]\n",
        "    grib = pygrib.open('/content/drive/MyDrive/lab_data/data/2011070103.grib2')\n",
        "    u_vert = np.array(grib.select()[2].values, dtype=np.float32)\n",
        "    src_dir = '/content/drive/MyDrive/lab_data/data/'\n",
        "    weather_dataset = WeatherDataset(src_dir, datetimeset, transform, target_transform, 1)\n",
        "    u_vert_direct = torch.from_numpy(u_vert).to(device)\n",
        "    u_vert_target = weather_dataset.__getitem__(0)['u_vert_target']\n",
        "    print(weather_dataset.__len__()) # 2\n",
        "    print(torch.equal(u_vert_target, u_vert_direct[1:-1, 1:-1] * -0.01)) # True\n",
        "    print(u_vert_direct.size())\n",
        "\n",
        "test_for_weatherdataset()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYE-xJkDExDu"
      },
      "source": [
        "### Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cEypdBd3EN28",
        "outputId": "65520f6c-5366-4fa9-d83c-c054642b89c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating a dataset and dataloader...\n",
            "100 samples collected, 8.45 s\n",
            "200 samples collected, 8.57 s\n",
            "300 samples collected, 5.20 s\n",
            "400 samples collected, 5.20 s\n",
            "500 samples collected, 5.16 s\n",
            "600 samples collected, 5.14 s\n",
            "700 samples collected, 5.95 s\n",
            "800 samples collected, 5.59 s\n",
            "900 samples collected, 5.32 s\n"
          ]
        }
      ],
      "source": [
        "def make_dataset_and_dataloader():\n",
        "    print('Creating a dataset and dataloader...')\n",
        "    data_dir = '/content/drive/MyDrive/lab_data/data'\n",
        "    datetimeset = [dt.datetime(2011+i, 7, 1, 0) + timedelta(days=j) for i in range(10) for j in range(92)]\n",
        "    dataset = WeatherDataset(data_dir, datetimeset, transform, target_transform, LBMwithNoisyInit.margin)\n",
        "    batch_size = 1\n",
        "    train_split = 0.8\n",
        "    shuffle_dataset = True # if false, this model will learn only by the 'past' data, and forcast the 'future' data.\n",
        "    random_seed = 42\n",
        "    \n",
        "    dataset_size =  len(dataset)\n",
        "    indices = list(range(dataset_size))\n",
        "    split = int(np.floor(train_split * dataset_size))\n",
        "    if shuffle_dataset:\n",
        "        np.random.seed(random_seed)\n",
        "        np.random.shuffle(indices)\n",
        "    train_indices, valid_indices = indices[:split], indices[split:]\n",
        "    \n",
        "    train_sampler = SubsetRandomSampler(train_indices)\n",
        "    valid_sampler = SubsetRandomSampler(valid_indices)\n",
        "    \n",
        "    train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler, num_workers=0)\n",
        "    valid_loader = DataLoader(dataset, batch_size=batch_size, sampler=valid_sampler, num_workers=0)\n",
        "\n",
        "    return dataset, train_loader, valid_loader\n",
        "\n",
        "dataset, train_loader, valid_loader = make_dataset_and_dataloader()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnq5AMAQFC_E"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVw9FGDxchOx"
      },
      "source": [
        "### Loss func"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C4tT4O7lsTiD"
      },
      "outputs": [],
      "source": [
        "def loss_func(pred: Dict[str, torch.Tensor], batch: Dict[str, torch.Tensor]):\n",
        "    # MSE\n",
        "    u_vert_loss = torch.sum(torch.square(pred['u_vert'] - batch['u_vert_target'].squeeze())) * U_MSE_COEF\n",
        "    u_hori_loss = torch.sum(torch.square(pred['u_hori'] - batch['u_hori_target'].squeeze())) * U_MSE_COEF\n",
        "    rho_loss = torch.sum(torch.square(pred['rho'] - batch['rho_target'].squeeze())) * RHO_MSE_COEF\n",
        "\n",
        "    # MAE\n",
        "    u_vert_mae = torch.sum(torch.abs(pred['u_vert'] - batch['u_vert_target'].squeeze())) / (U_COEF * HEIGHT * WIDTH) # actual MAE (m/s) per cell\n",
        "    u_hori_mae = torch.sum(torch.abs(pred['u_hori'] - batch['u_hori_target'].squeeze())) / (U_COEF * HEIGHT * WIDTH)\n",
        "    rho_mae = torch.sum(torch.abs(pred['rho'] - batch['rho_target'].squeeze())) / (RHO_COEF * HEIGHT * WIDTH * 100.) # hPa\n",
        "\n",
        "    return {\n",
        "        'u_vert': u_vert_loss,\n",
        "        'u_hori': u_hori_loss,\n",
        "        'rho': rho_loss,\n",
        "        'total': u_vert_loss + u_hori_loss + rho_loss,\n",
        "        'u_vert_mae': u_vert_mae,\n",
        "        'u_hori_mae': u_hori_mae,\n",
        "        'rho_mae': rho_mae\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5DW8w59ckQQ"
      },
      "source": [
        "### Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wV08pZahFfJE"
      },
      "outputs": [],
      "source": [
        "class WeightDiffLogger:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def memorize(self, lbm: LBMwithNoisyInit):\n",
        "        self.prev_tensors = {\n",
        "            'streaming1_w0': lbm.streaming1_layer.w0.clone(),\n",
        "            'streaming1_w1': lbm.streaming1_layer.w1.clone(),\n",
        "            'colliding1_w1': lbm.colliding1_layer.w1.clone(),\n",
        "            'colliding1_w2': lbm.colliding1_layer.w2.clone(),\n",
        "            'colliding1_w3': lbm.colliding1_layer.w3.clone(),\n",
        "            'colliding1_w4': lbm.colliding1_layer.w4.clone(),\n",
        "            'colliding1_w5': lbm.colliding1_layer.w5.clone(),\n",
        "            'streaming2_w0': lbm.streaming2_layer.w0.clone(),\n",
        "            'streaming2_w1': lbm.streaming2_layer.w1.clone(),\n",
        "            'colliding2_w1': lbm.colliding2_layer.w1.clone(),\n",
        "            'colliding2_w2': lbm.colliding2_layer.w2.clone(),\n",
        "            'colliding2_w3': lbm.colliding2_layer.w3.clone(),\n",
        "            'colliding2_w4': lbm.colliding2_layer.w4.clone(),\n",
        "            'colliding2_w5': lbm.colliding2_layer.w5.clone(),\n",
        "            'streaming3_w0': lbm.streaming3_layer.w0.clone(),\n",
        "            'streaming3_w1': lbm.streaming3_layer.w1.clone(),\n",
        "            'colliding3_w1': lbm.colliding3_layer.w1.clone(),\n",
        "            'colliding3_w2': lbm.colliding3_layer.w2.clone(),\n",
        "            'colliding3_w3': lbm.colliding3_layer.w3.clone(),\n",
        "            'colliding3_w4': lbm.colliding3_layer.w4.clone(),\n",
        "            'colliding3_w5': lbm.colliding3_layer.w5.clone(),\n",
        "            'streaming4_w0': lbm.streaming4_layer.w0.clone(),\n",
        "            'streaming4_w1': lbm.streaming4_layer.w1.clone(),\n",
        "            'colliding4_w1': lbm.colliding4_layer.w1.clone(),\n",
        "            'colliding4_w2': lbm.colliding4_layer.w2.clone(),\n",
        "            'colliding4_w3': lbm.colliding4_layer.w3.clone(),\n",
        "            'colliding4_w4': lbm.colliding4_layer.w4.clone(),\n",
        "            'colliding4_w5': lbm.colliding4_layer.w5.clone(),\n",
        "            'streaming5_w0': lbm.streaming5_layer.w0.clone(),\n",
        "            'streaming5_w1': lbm.streaming5_layer.w1.clone(),\n",
        "        }\n",
        "\n",
        "    def calc_and_log(self, lbm: LBMwithNoisyInit, writer: SummaryWriter, epoch: int):\n",
        "        with torch.no_grad():\n",
        "            writer.add_scalars(\n",
        "                'weight_diff',\n",
        "                {\n",
        "                    'streaming1_w0': torch.mean(torch.abs(self.prev_tensors['streaming1_w0'] - lbm.streaming1_layer.w0)),\n",
        "                    'streaming1_w1': torch.mean(torch.abs(self.prev_tensors['streaming1_w1'] - lbm.streaming1_layer.w1)),\n",
        "                    'colliding1_w1': torch.mean(torch.abs(self.prev_tensors['colliding1_w1'] - lbm.colliding1_layer.w1)),\n",
        "                    'colliding1_w2': torch.mean(torch.abs(self.prev_tensors['colliding1_w2'] - lbm.colliding1_layer.w2)),\n",
        "                    'colliding1_w3': torch.mean(torch.abs(self.prev_tensors['colliding1_w3'] - lbm.colliding1_layer.w3)),\n",
        "                    'colliding1_w4': torch.mean(torch.abs(self.prev_tensors['colliding1_w4'] - lbm.colliding1_layer.w4)),\n",
        "                    'colliding1_w5': torch.mean(torch.abs(self.prev_tensors['colliding1_w5'] - lbm.colliding1_layer.w5)),\n",
        "                    'streaming2_w0': torch.mean(torch.abs(self.prev_tensors['streaming2_w0'] - lbm.streaming2_layer.w0)),\n",
        "                    'streaming2_w1': torch.mean(torch.abs(self.prev_tensors['streaming2_w1'] - lbm.streaming2_layer.w1)),\n",
        "                    'colliding2_w1': torch.mean(torch.abs(self.prev_tensors['colliding2_w1'] - lbm.colliding2_layer.w1)),\n",
        "                    'colliding2_w2': torch.mean(torch.abs(self.prev_tensors['colliding2_w2'] - lbm.colliding2_layer.w2)),\n",
        "                    'colliding2_w3': torch.mean(torch.abs(self.prev_tensors['colliding2_w3'] - lbm.colliding2_layer.w3)),\n",
        "                    'colliding2_w4': torch.mean(torch.abs(self.prev_tensors['colliding2_w4'] - lbm.colliding2_layer.w4)),\n",
        "                    'colliding2_w5': torch.mean(torch.abs(self.prev_tensors['colliding2_w5'] - lbm.colliding2_layer.w5)),\n",
        "                    'streaming3_w0': torch.mean(torch.abs(self.prev_tensors['streaming3_w0'] - lbm.streaming3_layer.w0)),\n",
        "                    'streaming3_w1': torch.mean(torch.abs(self.prev_tensors['streaming3_w1'] - lbm.streaming3_layer.w1)),\n",
        "                    'colliding3_w1': torch.mean(torch.abs(self.prev_tensors['colliding3_w1'] - lbm.colliding3_layer.w1)),\n",
        "                    'colliding3_w2': torch.mean(torch.abs(self.prev_tensors['colliding3_w2'] - lbm.colliding3_layer.w2)),\n",
        "                    'colliding3_w3': torch.mean(torch.abs(self.prev_tensors['colliding3_w3'] - lbm.colliding3_layer.w3)),\n",
        "                    'colliding3_w4': torch.mean(torch.abs(self.prev_tensors['colliding3_w4'] - lbm.colliding3_layer.w4)),\n",
        "                    'colliding3_w5': torch.mean(torch.abs(self.prev_tensors['colliding3_w5'] - lbm.colliding3_layer.w5)),\n",
        "                    'streaming4_w0': torch.mean(torch.abs(self.prev_tensors['streaming4_w0'] - lbm.streaming4_layer.w0)),\n",
        "                    'streaming4_w1': torch.mean(torch.abs(self.prev_tensors['streaming4_w1'] - lbm.streaming4_layer.w1)),\n",
        "                    'colliding4_w1': torch.mean(torch.abs(self.prev_tensors['colliding4_w1'] - lbm.colliding4_layer.w1)),\n",
        "                    'colliding4_w2': torch.mean(torch.abs(self.prev_tensors['colliding4_w2'] - lbm.colliding4_layer.w2)),\n",
        "                    'colliding4_w3': torch.mean(torch.abs(self.prev_tensors['colliding4_w3'] - lbm.colliding4_layer.w3)),\n",
        "                    'colliding4_w4': torch.mean(torch.abs(self.prev_tensors['colliding4_w4'] - lbm.colliding4_layer.w4)),\n",
        "                    'colliding4_w5': torch.mean(torch.abs(self.prev_tensors['colliding4_w5'] - lbm.colliding4_layer.w5)),\n",
        "                    'streaming5_w0': torch.mean(torch.abs(self.prev_tensors['streaming5_w0'] - lbm.streaming5_layer.w0)),\n",
        "                    'streaming5_w1': torch.mean(torch.abs(self.prev_tensors['streaming5_w1'] - lbm.streaming5_layer.w1)),\n",
        "                },\n",
        "                epoch\n",
        "            )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ivMsz7HodASq"
      },
      "outputs": [],
      "source": [
        "def log_typhoon_forcast(pred: Dict[str, torch.Tensor], batch: Dict[str, torch.Tensor], writer: SummaryWriter, epoch: int):\n",
        "    if batch['datetime'][0] == '2020090200' and (epoch+1) % 5 == 0:\n",
        "        with torch.no_grad():\n",
        "            u_vert_in_trimmed, u_hori_in_trimmed, _ = remove_margin(batch['u_vert'].squeeze(), batch['u_hori'].squeeze(), batch['rho'].squeeze(), LBMwithNoisyInit.margin)\n",
        "            heatmap = torch.sqrt((pred['u_vert'] - batch['u_vert_target'].squeeze()) ** 2.0\\\n",
        "                                + (pred['u_hori'] - batch['u_hori_target'].squeeze()) ** 2.0)\\\n",
        "                    - torch.sqrt((u_vert_in_trimmed - batch['u_vert_target'].squeeze()) ** 2.0\\\n",
        "                                + (u_hori_in_trimmed - batch['u_hori_target'].squeeze()) ** 2.0)\n",
        "            fig = plt.figure()\n",
        "            sns.heatmap(heatmap.cpu().detach().numpy(), vmin=-0.0001, vmax=0.0001, cmap='bwr')\n",
        "            writer.add_figure(tag='typhoon forcast', figure=fig, global_step=epoch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZRWq1H3otPnu"
      },
      "outputs": [],
      "source": [
        "class Loss:\n",
        "    def __init__(self):\n",
        "        self.losses_train = {'u_vert': 0.0, 'u_hori': 0.0, 'rho': 0.0, 'total': 0.0, 'u_vert_mae': 0.0, 'u_hori_mae': 0.0, 'rho_mae': 0.0}\n",
        "        self.losses_val = {'u_vert': 0.0, 'u_hori': 0.0, 'rho': 0.0, 'total': 0.0, 'u_vert_mae': 0.0, 'u_hori_mae': 0.0, 'rho_mae': 0.0}\n",
        "        self.train_count = 0\n",
        "        self.val_count = 0\n",
        "\n",
        "    def add_losses_train(self, losses: Dict[str, torch.Tensor]):\n",
        "        self.train_count += 1\n",
        "        for k in self.losses_train.keys():\n",
        "            self.losses_train[k] += losses[k].item()\n",
        "\n",
        "    def add_losses_val(self, losses: Dict[str, torch.Tensor]):\n",
        "        self.val_count += 1\n",
        "        for k in self.losses_val.keys():\n",
        "            self.losses_val[k] += losses[k].item()\n",
        "\n",
        "    def get_losses_train(self):\n",
        "        return {\n",
        "            k: self.losses_train[k] / self.train_count for k in self.losses_train.keys()\n",
        "        }\n",
        "\n",
        "    def get_losses_val(self):\n",
        "        return {\n",
        "            k: self.losses_val[k] / self.val_count for k in self.losses_val.keys()\n",
        "        }\n",
        "\n",
        "    def log(self, writer: SummaryWriter, epoch: int, tag: str, keys: List[str], prefix: str = ''):\n",
        "        d = {}\n",
        "        losses_train = self.get_losses_train()\n",
        "        losses_val = self.get_losses_val()\n",
        "        for k in keys:\n",
        "            d[f'{prefix}train_{k}'] = losses_train[k]\n",
        "            d[f'{prefix}test_{k}'] = losses_val[k]\n",
        "\n",
        "        writer.add_scalars(\n",
        "            tag,\n",
        "            d,\n",
        "            epoch\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWgHQiVoBC8Q"
      },
      "source": [
        "### calc MAE of datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Y5B4A5FAwza"
      },
      "outputs": [],
      "source": [
        "def calc_losses_between_0h3h(train_loader, valid_loader):\n",
        "    loss = Loss()\n",
        "    for (batch_idx, batch) in enumerate(train_loader):\n",
        "        u_vert_input, u_hori_input, rho_input = remove_margin(\n",
        "            batch['u_vert'].squeeze(), batch['u_hori'].squeeze(), batch['rho'].squeeze(), LBMwithNoisyInit.margin\n",
        "        )\n",
        "        loss_vals = loss_func({\n",
        "            'u_vert': u_vert_input,\n",
        "            'u_hori': u_hori_input,\n",
        "            'rho': rho_input\n",
        "        }, batch)\n",
        "        loss.add_losses_train(loss_vals)\n",
        "\n",
        "    for (batch_idx, batch) in enumerate(valid_loader):\n",
        "        u_vert_input, u_hori_input, rho_input = remove_margin(\n",
        "            batch['u_vert'].squeeze(), batch['u_hori'].squeeze(), batch['rho'].squeeze(), LBMwithNoisyInit.margin\n",
        "        )\n",
        "        loss_vals = loss_func({\n",
        "            'u_vert': u_vert_input,\n",
        "            'u_hori': u_hori_input,\n",
        "            'rho': rho_input\n",
        "        }, batch)\n",
        "        loss.add_losses_val(loss_vals)\n",
        "\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KaV-5SghcogH"
      },
      "source": [
        "### Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQXqhgwqsitM",
        "outputId": "65d82790-64d2-4955-e6e2-88a4cbb28e86"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Calc losses between 0h and 3h...\n",
            "train u_vert mae: 1.1112427019878575 m/s\n",
            "train u_hori mae: 1.0288740297698455 m/s\n",
            "train rho mae: 0.7113683552521727 hPa\n",
            "val u_vert mae: 1.0980090366109558 m/s\n",
            "val u_hori mae: 1.0207934680840243 m/s\n",
            "val rho mae: 0.7064201222813647 hPa\n",
            "Creating the model...\n",
            "optimizer = Adam (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    capturable: False\n",
            "    differentiable: False\n",
            "    eps: 1e-08\n",
            "    foreach: None\n",
            "    fused: False\n",
            "    lr: 0.001\n",
            "    maximize: False\n",
            "    weight_decay: 0\n",
            ")\n",
            "max_epochs = 300 \n",
            "summary writer's logdir = /content/drive/MyDrive/lab_data/logs/20221231_150418\n",
            "Training the model...\n",
            "epoch =    0  train_loss = 1.6778e+06  val_loss = 1.6799e+06  u_vert_loss = 8.3647e+05  u_hori_loss = 7.6266e+05  rho_loss = 8.0752e+04  epoch time = 49.18s\n",
            "epoch =    1  train_loss = 1.6098e+06  val_loss = 1.5021e+06  u_vert_loss = 7.7440e+05  u_hori_loss = 6.5862e+05  rho_loss = 6.9090e+04  epoch time = 53.12s\n",
            "epoch =    2  train_loss = 1.5816e+06  val_loss = 1.6482e+06  u_vert_loss = 8.1134e+05  u_hori_loss = 7.5979e+05  rho_loss = 7.7121e+04  epoch time = 48.66s\n",
            "epoch =    3  train_loss = 1.5669e+06  val_loss = 1.4760e+06  u_vert_loss = 7.5037e+05  u_hori_loss = 6.5322e+05  rho_loss = 7.2393e+04  epoch time = 49.45s\n",
            "epoch =    4  train_loss = 1.5466e+06  val_loss = 1.6208e+06  u_vert_loss = 7.6361e+05  u_hori_loss = 7.7251e+05  rho_loss = 8.4632e+04  epoch time = 49.72s\n",
            "epoch =    5  train_loss = 1.5326e+06  val_loss = 1.5397e+06  u_vert_loss = 7.5243e+05  u_hori_loss = 7.0819e+05  rho_loss = 7.9034e+04  epoch time = 48.62s\n",
            "epoch =    6  train_loss = 1.5232e+06  val_loss = 1.4794e+06  u_vert_loss = 7.3703e+05  u_hori_loss = 6.7012e+05  rho_loss = 7.2236e+04  epoch time = 48.57s\n",
            "epoch =    7  train_loss = 1.5079e+06  val_loss = 1.5906e+06  u_vert_loss = 7.9371e+05  u_hori_loss = 7.2079e+05  rho_loss = 7.6087e+04  epoch time = 48.78s\n",
            "epoch =    8  train_loss = 1.4991e+06  val_loss = 1.4616e+06  u_vert_loss = 7.1383e+05  u_hori_loss = 6.7490e+05  rho_loss = 7.2905e+04  epoch time = 49.18s\n",
            "epoch =    9  train_loss = 1.4897e+06  val_loss = 1.4990e+06  u_vert_loss = 7.5856e+05  u_hori_loss = 6.6369e+05  rho_loss = 7.6763e+04  epoch time = 49.13s\n",
            "epoch =   10  train_loss = 1.4818e+06  val_loss = 1.3807e+06  u_vert_loss = 7.0399e+05  u_hori_loss = 6.0509e+05  rho_loss = 7.1621e+04  epoch time = 48.67s\n",
            "epoch =   11  train_loss = 1.4728e+06  val_loss = 1.4433e+06  u_vert_loss = 7.1299e+05  u_hori_loss = 6.5590e+05  rho_loss = 7.4463e+04  epoch time = 49.00s\n",
            "epoch =   12  train_loss = 1.4657e+06  val_loss = 1.3908e+06  u_vert_loss = 7.0395e+05  u_hori_loss = 6.1150e+05  rho_loss = 7.5304e+04  epoch time = 48.63s\n",
            "epoch =   13  train_loss = 1.4541e+06  val_loss = 1.4964e+06  u_vert_loss = 7.3622e+05  u_hori_loss = 6.8218e+05  rho_loss = 7.7960e+04  epoch time = 48.63s\n",
            "epoch =   14  train_loss = 1.4480e+06  val_loss = 1.4623e+06  u_vert_loss = 7.2630e+05  u_hori_loss = 6.5945e+05  rho_loss = 7.6565e+04  epoch time = 49.02s\n",
            "epoch =   15  train_loss = 1.4466e+06  val_loss = 1.4011e+06  u_vert_loss = 7.0772e+05  u_hori_loss = 6.1766e+05  rho_loss = 7.5740e+04  epoch time = 49.08s\n",
            "epoch =   16  train_loss = 1.4374e+06  val_loss = 1.3677e+06  u_vert_loss = 6.9152e+05  u_hori_loss = 6.0307e+05  rho_loss = 7.3067e+04  epoch time = 48.53s\n",
            "epoch =   17  train_loss = 1.4336e+06  val_loss = 1.4675e+06  u_vert_loss = 7.1592e+05  u_hori_loss = 6.6429e+05  rho_loss = 8.7334e+04  epoch time = 48.61s\n",
            "epoch =   18  train_loss = 1.4284e+06  val_loss = 1.4119e+06  u_vert_loss = 7.2274e+05  u_hori_loss = 6.1577e+05  rho_loss = 7.3429e+04  epoch time = 48.60s\n",
            "epoch =   19  train_loss = 1.4152e+06  val_loss = 1.3970e+06  u_vert_loss = 6.7684e+05  u_hori_loss = 6.4182e+05  rho_loss = 7.8329e+04  epoch time = 49.71s\n",
            "epoch =   20  train_loss = 1.4144e+06  val_loss = 1.4647e+06  u_vert_loss = 7.3024e+05  u_hori_loss = 6.5331e+05  rho_loss = 8.1134e+04  epoch time = 48.80s\n",
            "epoch =   21  train_loss = 1.4083e+06  val_loss = 1.4453e+06  u_vert_loss = 7.2317e+05  u_hori_loss = 6.4960e+05  rho_loss = 7.2523e+04  epoch time = 48.52s\n",
            "epoch =   22  train_loss = 1.4022e+06  val_loss = 1.4862e+06  u_vert_loss = 7.5214e+05  u_hori_loss = 6.5126e+05  rho_loss = 8.2755e+04  epoch time = 48.67s\n",
            "epoch =   23  train_loss = 1.3986e+06  val_loss = 1.3792e+06  u_vert_loss = 6.6572e+05  u_hori_loss = 6.3586e+05  rho_loss = 7.7622e+04  epoch time = 49.13s\n",
            "epoch =   24  train_loss = 1.3963e+06  val_loss = 1.4583e+06  u_vert_loss = 7.4328e+05  u_hori_loss = 6.3129e+05  rho_loss = 8.3687e+04  epoch time = 49.00s\n",
            "epoch =   25  train_loss = 1.3881e+06  val_loss = 1.4620e+06  u_vert_loss = 7.0401e+05  u_hori_loss = 6.7760e+05  rho_loss = 8.0375e+04  epoch time = 48.56s\n",
            "epoch =   26  train_loss = 1.3889e+06  val_loss = 1.3929e+06  u_vert_loss = 7.0239e+05  u_hori_loss = 6.1630e+05  rho_loss = 7.4223e+04  epoch time = 49.23s\n",
            "epoch =   27  train_loss = 1.3861e+06  val_loss = 1.3743e+06  u_vert_loss = 6.7889e+05  u_hori_loss = 6.2164e+05  rho_loss = 7.3724e+04  epoch time = 48.58s\n",
            "epoch =   28  train_loss = 1.3796e+06  val_loss = 1.3496e+06  u_vert_loss = 6.6999e+05  u_hori_loss = 6.0734e+05  rho_loss = 7.2228e+04  epoch time = 48.70s\n",
            "epoch =   29  train_loss = 1.3729e+06  val_loss = 1.3107e+06  u_vert_loss = 6.5592e+05  u_hori_loss = 5.8425e+05  rho_loss = 7.0478e+04  epoch time = 49.06s\n",
            "epoch =   30  train_loss = 1.3745e+06  val_loss = 1.4258e+06  u_vert_loss = 7.1786e+05  u_hori_loss = 6.3087e+05  rho_loss = 7.7110e+04  epoch time = 49.10s\n",
            "epoch =   31  train_loss = 1.3743e+06  val_loss = 1.3980e+06  u_vert_loss = 6.9524e+05  u_hori_loss = 6.3150e+05  rho_loss = 7.1239e+04  epoch time = 48.64s\n",
            "epoch =   32  train_loss = 1.3619e+06  val_loss = 1.4831e+06  u_vert_loss = 7.2461e+05  u_hori_loss = 6.7855e+05  rho_loss = 7.9945e+04  epoch time = 48.57s\n",
            "epoch =   33  train_loss = 1.3625e+06  val_loss = 1.3721e+06  u_vert_loss = 6.7677e+05  u_hori_loss = 6.2190e+05  rho_loss = 7.3399e+04  epoch time = 48.51s\n",
            "epoch =   34  train_loss = 1.3599e+06  val_loss = 1.3338e+06  u_vert_loss = 6.6474e+05  u_hori_loss = 5.9615e+05  rho_loss = 7.2952e+04  epoch time = 49.50s\n",
            "epoch =   35  train_loss = 1.3576e+06  val_loss = 1.3810e+06  u_vert_loss = 6.8694e+05  u_hori_loss = 6.1527e+05  rho_loss = 7.8781e+04  epoch time = 48.49s\n",
            "epoch =   36  train_loss = 1.3526e+06  val_loss = 1.3504e+06  u_vert_loss = 6.6191e+05  u_hori_loss = 6.0668e+05  rho_loss = 8.1804e+04  epoch time = 48.48s\n",
            "epoch =   37  train_loss = 1.3520e+06  val_loss = 1.3452e+06  u_vert_loss = 6.6979e+05  u_hori_loss = 6.0233e+05  rho_loss = 7.3102e+04  epoch time = 48.55s\n",
            "epoch =   38  train_loss = 1.3517e+06  val_loss = 1.3062e+06  u_vert_loss = 6.5453e+05  u_hori_loss = 5.7889e+05  rho_loss = 7.2784e+04  epoch time = 48.91s\n",
            "epoch =   39  train_loss = 1.3444e+06  val_loss = 1.3847e+06  u_vert_loss = 7.0300e+05  u_hori_loss = 6.0941e+05  rho_loss = 7.2318e+04  epoch time = 49.05s\n",
            "epoch =   40  train_loss = 1.3433e+06  val_loss = 1.3460e+06  u_vert_loss = 6.6464e+05  u_hori_loss = 6.0823e+05  rho_loss = 7.3111e+04  epoch time = 48.37s\n",
            "epoch =   41  train_loss = 1.3396e+06  val_loss = 1.2971e+06  u_vert_loss = 6.4738e+05  u_hori_loss = 5.8370e+05  rho_loss = 6.6039e+04  epoch time = 48.29s\n",
            "epoch =   42  train_loss = 1.3390e+06  val_loss = 1.3787e+06  u_vert_loss = 6.8719e+05  u_hori_loss = 6.1467e+05  rho_loss = 7.6872e+04  epoch time = 48.69s\n",
            "epoch =   43  train_loss = 1.3352e+06  val_loss = 1.3477e+06  u_vert_loss = 6.7107e+05  u_hori_loss = 6.0286e+05  rho_loss = 7.3762e+04  epoch time = 48.28s\n",
            "epoch =   44  train_loss = 1.3386e+06  val_loss = 1.3388e+06  u_vert_loss = 6.5474e+05  u_hori_loss = 6.0044e+05  rho_loss = 8.3582e+04  epoch time = 48.78s\n",
            "epoch =   45  train_loss = 1.3285e+06  val_loss = 1.2785e+06  u_vert_loss = 6.4269e+05  u_hori_loss = 5.6637e+05  rho_loss = 6.9422e+04  epoch time = 48.40s\n",
            "epoch =   46  train_loss = 1.3284e+06  val_loss = 1.3234e+06  u_vert_loss = 6.5520e+05  u_hori_loss = 5.9528e+05  rho_loss = 7.2938e+04  epoch time = 48.61s\n",
            "epoch =   47  train_loss = 1.3260e+06  val_loss = 1.3477e+06  u_vert_loss = 6.5672e+05  u_hori_loss = 6.1542e+05  rho_loss = 7.5595e+04  epoch time = 48.30s\n",
            "epoch =   48  train_loss = 1.3284e+06  val_loss = 1.2883e+06  u_vert_loss = 6.4965e+05  u_hori_loss = 5.6671e+05  rho_loss = 7.1965e+04  epoch time = 48.20s\n",
            "epoch =   49  train_loss = 1.3212e+06  val_loss = 1.3691e+06  u_vert_loss = 6.6396e+05  u_hori_loss = 6.3059e+05  rho_loss = 7.4540e+04  epoch time = 48.68s\n",
            "epoch =   50  train_loss = 1.3221e+06  val_loss = 1.3145e+06  u_vert_loss = 6.5970e+05  u_hori_loss = 5.8210e+05  rho_loss = 7.2708e+04  epoch time = 48.80s\n",
            "epoch =   51  train_loss = 1.3148e+06  val_loss = 1.3137e+06  u_vert_loss = 6.4160e+05  u_hori_loss = 5.9525e+05  rho_loss = 7.6873e+04  epoch time = 48.29s\n",
            "epoch =   52  train_loss = 1.3176e+06  val_loss = 1.3565e+06  u_vert_loss = 6.5500e+05  u_hori_loss = 6.2425e+05  rho_loss = 7.7224e+04  epoch time = 48.39s\n",
            "epoch =   53  train_loss = 1.3171e+06  val_loss = 1.2667e+06  u_vert_loss = 6.3315e+05  u_hori_loss = 5.6198e+05  rho_loss = 7.1614e+04  epoch time = 48.34s\n",
            "epoch =   54  train_loss = 1.3127e+06  val_loss = 1.3177e+06  u_vert_loss = 6.5570e+05  u_hori_loss = 5.8514e+05  rho_loss = 7.6855e+04  epoch time = 49.05s\n",
            "epoch =   55  train_loss = 1.3105e+06  val_loss = 1.2974e+06  u_vert_loss = 6.4342e+05  u_hori_loss = 5.8185e+05  rho_loss = 7.2112e+04  epoch time = 48.45s\n",
            "epoch =   56  train_loss = 1.3094e+06  val_loss = 1.3056e+06  u_vert_loss = 6.3835e+05  u_hori_loss = 5.9618e+05  rho_loss = 7.1086e+04  epoch time = 48.21s\n",
            "epoch =   57  train_loss = 1.3023e+06  val_loss = 1.3458e+06  u_vert_loss = 6.4813e+05  u_hori_loss = 6.1151e+05  rho_loss = 8.6129e+04  epoch time = 48.33s\n",
            "epoch =   58  train_loss = 1.3084e+06  val_loss = 1.3337e+06  u_vert_loss = 6.5862e+05  u_hori_loss = 5.9329e+05  rho_loss = 8.1752e+04  epoch time = 48.69s\n",
            "epoch =   59  train_loss = 1.3036e+06  val_loss = 1.3107e+06  u_vert_loss = 6.4956e+05  u_hori_loss = 5.8425e+05  rho_loss = 7.6836e+04  epoch time = 48.61s\n",
            "epoch =   60  train_loss = 1.3023e+06  val_loss = 1.2929e+06  u_vert_loss = 6.5194e+05  u_hori_loss = 5.7125e+05  rho_loss = 6.9703e+04  epoch time = 48.29s\n",
            "epoch =   61  train_loss = 1.3021e+06  val_loss = 1.2746e+06  u_vert_loss = 6.2208e+05  u_hori_loss = 5.7815e+05  rho_loss = 7.4323e+04  epoch time = 48.16s\n",
            "epoch =   62  train_loss = 1.2989e+06  val_loss = 1.2661e+06  u_vert_loss = 6.3463e+05  u_hori_loss = 5.5891e+05  rho_loss = 7.2579e+04  epoch time = 48.75s\n",
            "epoch =   63  train_loss = 1.2972e+06  val_loss = 1.3274e+06  u_vert_loss = 6.4585e+05  u_hori_loss = 5.9674e+05  rho_loss = 8.4777e+04  epoch time = 48.33s\n",
            "epoch =   64  train_loss = 1.2968e+06  val_loss = 1.2752e+06  u_vert_loss = 6.3870e+05  u_hori_loss = 5.6800e+05  rho_loss = 6.8488e+04  epoch time = 48.84s\n",
            "epoch =   65  train_loss = 1.2952e+06  val_loss = 1.2823e+06  u_vert_loss = 6.3603e+05  u_hori_loss = 5.7372e+05  rho_loss = 7.2512e+04  epoch time = 48.85s\n",
            "epoch =   66  train_loss = 1.2922e+06  val_loss = 1.2740e+06  u_vert_loss = 6.2607e+05  u_hori_loss = 5.7588e+05  rho_loss = 7.2039e+04  epoch time = 48.52s\n",
            "epoch =   67  train_loss = 1.2887e+06  val_loss = 1.2956e+06  u_vert_loss = 6.4468e+05  u_hori_loss = 5.7971e+05  rho_loss = 7.1221e+04  epoch time = 48.58s\n",
            "epoch =   68  train_loss = 1.2938e+06  val_loss = 1.2810e+06  u_vert_loss = 6.3414e+05  u_hori_loss = 5.7271e+05  rho_loss = 7.4170e+04  epoch time = 49.34s\n",
            "epoch =   69  train_loss = 1.2933e+06  val_loss = 1.3074e+06  u_vert_loss = 6.4624e+05  u_hori_loss = 5.9083e+05  rho_loss = 7.0330e+04  epoch time = 50.01s\n",
            "epoch =   70  train_loss = 1.2900e+06  val_loss = 1.3097e+06  u_vert_loss = 6.4474e+05  u_hori_loss = 5.9111e+05  rho_loss = 7.3805e+04  epoch time = 48.86s\n",
            "epoch =   71  train_loss = 1.2872e+06  val_loss = 1.2730e+06  u_vert_loss = 6.4049e+05  u_hori_loss = 5.6172e+05  rho_loss = 7.0791e+04  epoch time = 49.04s\n",
            "epoch =   72  train_loss = 1.2837e+06  val_loss = 1.3321e+06  u_vert_loss = 6.5704e+05  u_hori_loss = 5.9604e+05  rho_loss = 7.9024e+04  epoch time = 49.84s\n",
            "epoch =   73  train_loss = 1.2813e+06  val_loss = 1.2576e+06  u_vert_loss = 6.2867e+05  u_hori_loss = 5.5745e+05  rho_loss = 7.1485e+04  epoch time = 49.04s\n",
            "epoch =   74  train_loss = 1.2827e+06  val_loss = 1.2851e+06  u_vert_loss = 6.4190e+05  u_hori_loss = 5.6437e+05  rho_loss = 7.8801e+04  epoch time = 49.59s\n",
            "epoch =   75  train_loss = 1.2815e+06  val_loss = 1.2789e+06  u_vert_loss = 6.3280e+05  u_hori_loss = 5.7150e+05  rho_loss = 7.4610e+04  epoch time = 49.00s\n",
            "epoch =   76  train_loss = 1.2850e+06  val_loss = 1.2620e+06  u_vert_loss = 6.2924e+05  u_hori_loss = 5.6112e+05  rho_loss = 7.1657e+04  epoch time = 49.20s\n",
            "epoch =   77  train_loss = 1.2786e+06  val_loss = 1.2871e+06  u_vert_loss = 6.4680e+05  u_hori_loss = 5.6710e+05  rho_loss = 7.3220e+04  epoch time = 48.62s\n",
            "epoch =   78  train_loss = 1.2799e+06  val_loss = 1.3293e+06  u_vert_loss = 6.3971e+05  u_hori_loss = 6.0001e+05  rho_loss = 8.9588e+04  epoch time = 48.52s\n",
            "epoch =   79  train_loss = 1.2797e+06  val_loss = 1.2742e+06  u_vert_loss = 6.3621e+05  u_hori_loss = 5.6839e+05  rho_loss = 6.9567e+04  epoch time = 49.70s\n",
            "epoch =   80  train_loss = 1.2782e+06  val_loss = 1.2834e+06  u_vert_loss = 6.3860e+05  u_hori_loss = 5.7241e+05  rho_loss = 7.2364e+04  epoch time = 48.34s\n",
            "epoch =   81  train_loss = 1.2749e+06  val_loss = 1.2948e+06  u_vert_loss = 6.3332e+05  u_hori_loss = 5.8886e+05  rho_loss = 7.2655e+04  epoch time = 48.43s\n",
            "epoch =   82  train_loss = 1.2714e+06  val_loss = 1.3063e+06  u_vert_loss = 6.4378e+05  u_hori_loss = 5.8622e+05  rho_loss = 7.6259e+04  epoch time = 48.49s\n",
            "epoch =   83  train_loss = 1.2731e+06  val_loss = 1.2451e+06  u_vert_loss = 6.1667e+05  u_hori_loss = 5.6179e+05  rho_loss = 6.6690e+04  epoch time = 48.84s\n",
            "epoch =   84  train_loss = 1.2732e+06  val_loss = 1.2839e+06  u_vert_loss = 6.3537e+05  u_hori_loss = 5.7428e+05  rho_loss = 7.4260e+04  epoch time = 48.95s\n",
            "epoch =   85  train_loss = 1.2684e+06  val_loss = 1.2377e+06  u_vert_loss = 6.1625e+05  u_hori_loss = 5.5121e+05  rho_loss = 7.0287e+04  epoch time = 48.40s\n",
            "epoch =   86  train_loss = 1.2692e+06  val_loss = 1.2473e+06  u_vert_loss = 6.1928e+05  u_hori_loss = 5.5344e+05  rho_loss = 7.4594e+04  epoch time = 48.85s\n",
            "epoch =   87  train_loss = 1.2710e+06  val_loss = 1.2945e+06  u_vert_loss = 6.4560e+05  u_hori_loss = 5.7322e+05  rho_loss = 7.5649e+04  epoch time = 48.49s\n",
            "epoch =   88  train_loss = 1.2678e+06  val_loss = 1.2877e+06  u_vert_loss = 6.2740e+05  u_hori_loss = 5.9339e+05  rho_loss = 6.6937e+04  epoch time = 48.33s\n",
            "epoch =   89  train_loss = 1.2666e+06  val_loss = 1.2477e+06  u_vert_loss = 6.2157e+05  u_hori_loss = 5.5586e+05  rho_loss = 7.0251e+04  epoch time = 48.89s\n",
            "epoch =   90  train_loss = 1.2655e+06  val_loss = 1.2559e+06  u_vert_loss = 6.3178e+05  u_hori_loss = 5.5479e+05  rho_loss = 6.9335e+04  epoch time = 48.78s\n",
            "epoch =   91  train_loss = 1.2665e+06  val_loss = 1.2559e+06  u_vert_loss = 6.2323e+05  u_hori_loss = 5.5696e+05  rho_loss = 7.5688e+04  epoch time = 48.57s\n",
            "epoch =   92  train_loss = 1.2649e+06  val_loss = 1.2500e+06  u_vert_loss = 6.2197e+05  u_hori_loss = 5.5798e+05  rho_loss = 7.0048e+04  epoch time = 48.41s\n",
            "epoch =   93  train_loss = 1.2622e+06  val_loss = 1.2502e+06  u_vert_loss = 6.2170e+05  u_hori_loss = 5.5700e+05  rho_loss = 7.1552e+04  epoch time = 48.80s\n",
            "epoch =   94  train_loss = 1.2638e+06  val_loss = 1.3065e+06  u_vert_loss = 6.4498e+05  u_hori_loss = 5.8583e+05  rho_loss = 7.5705e+04  epoch time = 48.91s\n",
            "epoch =   95  train_loss = 1.2625e+06  val_loss = 1.2748e+06  u_vert_loss = 6.3869e+05  u_hori_loss = 5.5969e+05  rho_loss = 7.6389e+04  epoch time = 48.49s\n",
            "epoch =   96  train_loss = 1.2616e+06  val_loss = 1.2483e+06  u_vert_loss = 6.2084e+05  u_hori_loss = 5.5824e+05  rho_loss = 6.9209e+04  epoch time = 48.46s\n",
            "epoch =   97  train_loss = 1.2629e+06  val_loss = 1.2603e+06  u_vert_loss = 6.2555e+05  u_hori_loss = 5.6292e+05  rho_loss = 7.1799e+04  epoch time = 48.48s\n",
            "epoch =   98  train_loss = 1.2598e+06  val_loss = 1.2666e+06  u_vert_loss = 6.2556e+05  u_hori_loss = 5.6819e+05  rho_loss = 7.2841e+04  epoch time = 48.80s\n",
            "epoch =   99  train_loss = 1.2575e+06  val_loss = 1.2315e+06  u_vert_loss = 6.0857e+05  u_hori_loss = 5.4970e+05  rho_loss = 7.3223e+04  epoch time = 48.82s\n",
            "epoch =  100  train_loss = 1.2579e+06  val_loss = 1.2341e+06  u_vert_loss = 6.1405e+05  u_hori_loss = 5.5326e+05  rho_loss = 6.6760e+04  epoch time = 48.76s\n",
            "epoch =  101  train_loss = 1.2564e+06  val_loss = 1.2542e+06  u_vert_loss = 6.2560e+05  u_hori_loss = 5.5351e+05  rho_loss = 7.5116e+04  epoch time = 48.41s\n",
            "epoch =  102  train_loss = 1.2539e+06  val_loss = 1.2489e+06  u_vert_loss = 6.2313e+05  u_hori_loss = 5.5219e+05  rho_loss = 7.3612e+04  epoch time = 48.38s\n",
            "epoch =  103  train_loss = 1.2522e+06  val_loss = 1.2718e+06  u_vert_loss = 6.4504e+05  u_hori_loss = 5.5379e+05  rho_loss = 7.2999e+04  epoch time = 48.49s\n",
            "epoch =  104  train_loss = 1.2547e+06  val_loss = 1.2746e+06  u_vert_loss = 6.3401e+05  u_hori_loss = 5.6023e+05  rho_loss = 8.0323e+04  epoch time = 49.19s\n",
            "epoch =  105  train_loss = 1.2505e+06  val_loss = 1.2942e+06  u_vert_loss = 6.3589e+05  u_hori_loss = 5.8660e+05  rho_loss = 7.1729e+04  epoch time = 48.46s\n",
            "epoch =  106  train_loss = 1.2529e+06  val_loss = 1.2533e+06  u_vert_loss = 6.2452e+05  u_hori_loss = 5.5515e+05  rho_loss = 7.3632e+04  epoch time = 48.46s\n",
            "epoch =  107  train_loss = 1.2480e+06  val_loss = 1.2500e+06  u_vert_loss = 6.1775e+05  u_hori_loss = 5.6013e+05  rho_loss = 7.2160e+04  epoch time = 48.69s\n",
            "epoch =  108  train_loss = 1.2526e+06  val_loss = 1.2303e+06  u_vert_loss = 6.0984e+05  u_hori_loss = 5.5240e+05  rho_loss = 6.8104e+04  epoch time = 48.34s\n",
            "epoch =  109  train_loss = 1.2517e+06  val_loss = 1.2411e+06  u_vert_loss = 6.1364e+05  u_hori_loss = 5.5296e+05  rho_loss = 7.4492e+04  epoch time = 48.75s\n",
            "epoch =  110  train_loss = 1.2510e+06  val_loss = 1.3174e+06  u_vert_loss = 6.3967e+05  u_hori_loss = 5.9347e+05  rho_loss = 8.4230e+04  epoch time = 48.78s\n",
            "epoch =  111  train_loss = 1.2475e+06  val_loss = 1.2348e+06  u_vert_loss = 6.1571e+05  u_hori_loss = 5.4849e+05  rho_loss = 7.0642e+04  epoch time = 48.30s\n",
            "epoch =  112  train_loss = 1.2478e+06  val_loss = 1.2551e+06  u_vert_loss = 6.2303e+05  u_hori_loss = 5.5819e+05  rho_loss = 7.3932e+04  epoch time = 48.28s\n",
            "epoch =  113  train_loss = 1.2475e+06  val_loss = 1.2430e+06  u_vert_loss = 6.2074e+05  u_hori_loss = 5.5193e+05  rho_loss = 7.0366e+04  epoch time = 48.71s\n",
            "epoch =  114  train_loss = 1.2416e+06  val_loss = 1.2717e+06  u_vert_loss = 6.3570e+05  u_hori_loss = 5.6153e+05  rho_loss = 7.4435e+04  epoch time = 48.83s\n",
            "epoch =  115  train_loss = 1.2436e+06  val_loss = 1.2561e+06  u_vert_loss = 6.1708e+05  u_hori_loss = 5.6362e+05  rho_loss = 7.5382e+04  epoch time = 48.30s\n",
            "epoch =  116  train_loss = 1.2454e+06  val_loss = 1.2395e+06  u_vert_loss = 6.0478e+05  u_hori_loss = 5.6405e+05  rho_loss = 7.0693e+04  epoch time = 48.77s\n",
            "epoch =  117  train_loss = 1.2440e+06  val_loss = 1.2257e+06  u_vert_loss = 6.1363e+05  u_hori_loss = 5.4434e+05  rho_loss = 6.7694e+04  epoch time = 48.37s\n",
            "epoch =  118  train_loss = 1.2446e+06  val_loss = 1.2415e+06  u_vert_loss = 6.1966e+05  u_hori_loss = 5.4867e+05  rho_loss = 7.3140e+04  epoch time = 48.43s\n",
            "epoch =  119  train_loss = 1.2422e+06  val_loss = 1.3157e+06  u_vert_loss = 6.3748e+05  u_hori_loss = 5.9552e+05  rho_loss = 8.2702e+04  epoch time = 49.29s\n",
            "epoch =  120  train_loss = 1.2464e+06  val_loss = 1.2490e+06  u_vert_loss = 6.1711e+05  u_hori_loss = 5.5680e+05  rho_loss = 7.5089e+04  epoch time = 48.41s\n",
            "epoch =  121  train_loss = 1.2406e+06  val_loss = 1.2375e+06  u_vert_loss = 6.1526e+05  u_hori_loss = 5.4923e+05  rho_loss = 7.3010e+04  epoch time = 48.37s\n",
            "epoch =  122  train_loss = 1.2389e+06  val_loss = 1.2566e+06  u_vert_loss = 6.2100e+05  u_hori_loss = 5.6081e+05  rho_loss = 7.4757e+04  epoch time = 48.77s\n",
            "epoch =  123  train_loss = 1.2405e+06  val_loss = 1.2454e+06  u_vert_loss = 6.1915e+05  u_hori_loss = 5.5103e+05  rho_loss = 7.5265e+04  epoch time = 48.40s\n",
            "epoch =  124  train_loss = 1.2391e+06  val_loss = 1.2547e+06  u_vert_loss = 6.2515e+05  u_hori_loss = 5.5482e+05  rho_loss = 7.4767e+04  epoch time = 48.84s\n",
            "epoch =  125  train_loss = 1.2375e+06  val_loss = 1.2433e+06  u_vert_loss = 6.2922e+05  u_hori_loss = 5.4767e+05  rho_loss = 6.6366e+04  epoch time = 48.67s\n",
            "epoch =  126  train_loss = 1.2377e+06  val_loss = 1.2536e+06  u_vert_loss = 6.2825e+05  u_hori_loss = 5.5682e+05  rho_loss = 6.8526e+04  epoch time = 48.52s\n",
            "epoch =  127  train_loss = 1.2339e+06  val_loss = 1.2891e+06  u_vert_loss = 6.1734e+05  u_hori_loss = 5.8797e+05  rho_loss = 8.3782e+04  epoch time = 48.41s\n",
            "epoch =  128  train_loss = 1.2366e+06  val_loss = 1.2217e+06  u_vert_loss = 6.0229e+05  u_hori_loss = 5.4764e+05  rho_loss = 7.1753e+04  epoch time = 48.73s\n",
            "epoch =  129  train_loss = 1.2375e+06  val_loss = 1.2176e+06  u_vert_loss = 6.0157e+05  u_hori_loss = 5.4498e+05  rho_loss = 7.1022e+04  epoch time = 48.80s\n",
            "epoch =  130  train_loss = 1.2372e+06  val_loss = 1.2468e+06  u_vert_loss = 6.1519e+05  u_hori_loss = 5.5613e+05  rho_loss = 7.5473e+04  epoch time = 48.31s\n",
            "epoch =  131  train_loss = 1.2350e+06  val_loss = 1.2514e+06  u_vert_loss = 6.2714e+05  u_hori_loss = 5.5197e+05  rho_loss = 7.2287e+04  epoch time = 48.77s\n",
            "epoch =  132  train_loss = 1.2370e+06  val_loss = 1.2273e+06  u_vert_loss = 6.0470e+05  u_hori_loss = 5.4866e+05  rho_loss = 7.3922e+04  epoch time = 48.30s\n",
            "epoch =  133  train_loss = 1.2322e+06  val_loss = 1.2340e+06  u_vert_loss = 6.1183e+05  u_hori_loss = 5.5012e+05  rho_loss = 7.2034e+04  epoch time = 48.42s\n",
            "epoch =  134  train_loss = 1.2347e+06  val_loss = 1.2361e+06  u_vert_loss = 6.1670e+05  u_hori_loss = 5.5341e+05  rho_loss = 6.5975e+04  epoch time = 49.41s\n",
            "epoch =  135  train_loss = 1.2289e+06  val_loss = 1.2326e+06  u_vert_loss = 6.1782e+05  u_hori_loss = 5.4328e+05  rho_loss = 7.1469e+04  epoch time = 48.42s\n",
            "epoch =  136  train_loss = 1.2318e+06  val_loss = 1.2918e+06  u_vert_loss = 6.4482e+05  u_hori_loss = 5.7756e+05  rho_loss = 6.9426e+04  epoch time = 48.37s\n",
            "epoch =  137  train_loss = 1.2311e+06  val_loss = 1.2172e+06  u_vert_loss = 6.0702e+05  u_hori_loss = 5.3724e+05  rho_loss = 7.2969e+04  epoch time = 48.68s\n",
            "epoch =  138  train_loss = 1.2299e+06  val_loss = 1.2755e+06  u_vert_loss = 6.2604e+05  u_hori_loss = 5.7517e+05  rho_loss = 7.4274e+04  epoch time = 48.35s\n",
            "epoch =  139  train_loss = 1.2307e+06  val_loss = 1.2636e+06  u_vert_loss = 6.2219e+05  u_hori_loss = 5.6817e+05  rho_loss = 7.3260e+04  epoch time = 49.16s\n",
            "epoch =  140  train_loss = 1.2296e+06  val_loss = 1.2222e+06  u_vert_loss = 6.1057e+05  u_hori_loss = 5.4103e+05  rho_loss = 7.0554e+04  epoch time = 48.53s\n",
            "epoch =  141  train_loss = 1.2266e+06  val_loss = 1.2240e+06  u_vert_loss = 6.1125e+05  u_hori_loss = 5.4635e+05  rho_loss = 6.6417e+04  epoch time = 48.35s\n",
            "epoch =  142  train_loss = 1.2285e+06  val_loss = 1.2490e+06  u_vert_loss = 6.2223e+05  u_hori_loss = 5.5706e+05  rho_loss = 6.9690e+04  epoch time = 48.68s\n",
            "epoch =  143  train_loss = 1.2284e+06  val_loss = 1.2719e+06  u_vert_loss = 6.2354e+05  u_hori_loss = 5.6920e+05  rho_loss = 7.9153e+04  epoch time = 48.40s\n",
            "epoch =  144  train_loss = 1.2269e+06  val_loss = 1.2772e+06  u_vert_loss = 6.4457e+05  u_hori_loss = 5.5885e+05  rho_loss = 7.3817e+04  epoch time = 48.78s\n",
            "epoch =  145  train_loss = 1.2255e+06  val_loss = 1.2331e+06  u_vert_loss = 6.0751e+05  u_hori_loss = 5.5533e+05  rho_loss = 7.0237e+04  epoch time = 48.73s\n",
            "epoch =  146  train_loss = 1.2241e+06  val_loss = 1.2351e+06  u_vert_loss = 6.0524e+05  u_hori_loss = 5.5137e+05  rho_loss = 7.8532e+04  epoch time = 48.33s\n",
            "epoch =  147  train_loss = 1.2266e+06  val_loss = 1.2080e+06  u_vert_loss = 6.0110e+05  u_hori_loss = 5.3991e+05  rho_loss = 6.6976e+04  epoch time = 48.30s\n",
            "epoch =  148  train_loss = 1.2274e+06  val_loss = 1.2396e+06  u_vert_loss = 6.1330e+05  u_hori_loss = 5.5430e+05  rho_loss = 7.1959e+04  epoch time = 48.72s\n",
            "epoch =  149  train_loss = 1.2255e+06  val_loss = 1.2267e+06  u_vert_loss = 6.0607e+05  u_hori_loss = 5.5072e+05  rho_loss = 6.9936e+04  epoch time = 48.80s\n",
            "epoch =  150  train_loss = 1.2252e+06  val_loss = 1.2278e+06  u_vert_loss = 6.1538e+05  u_hori_loss = 5.4569e+05  rho_loss = 6.6699e+04  epoch time = 48.75s\n",
            "epoch =  151  train_loss = 1.2258e+06  val_loss = 1.2448e+06  u_vert_loss = 6.1625e+05  u_hori_loss = 5.5909e+05  rho_loss = 6.9463e+04  epoch time = 48.39s\n",
            "epoch =  152  train_loss = 1.2243e+06  val_loss = 1.2190e+06  u_vert_loss = 5.9555e+05  u_hori_loss = 5.5180e+05  rho_loss = 7.1702e+04  epoch time = 48.35s\n",
            "epoch =  153  train_loss = 1.2222e+06  val_loss = 1.2389e+06  u_vert_loss = 6.1116e+05  u_hori_loss = 5.5425e+05  rho_loss = 7.3503e+04  epoch time = 48.75s\n",
            "epoch =  154  train_loss = 1.2202e+06  val_loss = 1.2471e+06  u_vert_loss = 6.1100e+05  u_hori_loss = 5.5968e+05  rho_loss = 7.6450e+04  epoch time = 48.74s\n",
            "epoch =  155  train_loss = 1.2215e+06  val_loss = 1.2225e+06  u_vert_loss = 6.0286e+05  u_hori_loss = 5.4385e+05  rho_loss = 7.5739e+04  epoch time = 48.57s\n",
            "epoch =  156  train_loss = 1.2215e+06  val_loss = 1.2169e+06  u_vert_loss = 6.0082e+05  u_hori_loss = 5.4720e+05  rho_loss = 6.8927e+04  epoch time = 48.72s\n",
            "epoch =  157  train_loss = 1.2211e+06  val_loss = 1.2303e+06  u_vert_loss = 6.0658e+05  u_hori_loss = 5.4755e+05  rho_loss = 7.6139e+04  epoch time = 48.31s\n",
            "epoch =  158  train_loss = 1.2209e+06  val_loss = 1.2284e+06  u_vert_loss = 6.0967e+05  u_hori_loss = 5.4773e+05  rho_loss = 7.0954e+04  epoch time = 48.71s\n",
            "epoch =  159  train_loss = 1.2190e+06  val_loss = 1.2517e+06  u_vert_loss = 6.0969e+05  u_hori_loss = 5.6618e+05  rho_loss = 7.5847e+04  epoch time = 48.86s\n",
            "epoch =  160  train_loss = 1.2194e+06  val_loss = 1.2246e+06  u_vert_loss = 6.0731e+05  u_hori_loss = 5.4552e+05  rho_loss = 7.1746e+04  epoch time = 48.34s\n",
            "epoch =  161  train_loss = 1.2189e+06  val_loss = 1.2259e+06  u_vert_loss = 6.0692e+05  u_hori_loss = 5.4812e+05  rho_loss = 7.0817e+04  epoch time = 48.76s\n",
            "epoch =  162  train_loss = 1.2177e+06  val_loss = 1.2404e+06  u_vert_loss = 6.2305e+05  u_hori_loss = 5.4737e+05  rho_loss = 6.9943e+04  epoch time = 48.26s\n",
            "epoch =  163  train_loss = 1.2148e+06  val_loss = 1.2380e+06  u_vert_loss = 6.2175e+05  u_hori_loss = 5.3901e+05  rho_loss = 7.7278e+04  epoch time = 48.31s\n",
            "epoch =  164  train_loss = 1.2165e+06  val_loss = 1.2740e+06  u_vert_loss = 6.2867e+05  u_hori_loss = 5.5251e+05  rho_loss = 9.2858e+04  epoch time = 49.17s\n",
            "epoch =  165  train_loss = 1.2199e+06  val_loss = 1.2094e+06  u_vert_loss = 5.9587e+05  u_hori_loss = 5.3852e+05  rho_loss = 7.5009e+04  epoch time = 48.32s\n",
            "epoch =  166  train_loss = 1.2154e+06  val_loss = 1.2074e+06  u_vert_loss = 5.9783e+05  u_hori_loss = 5.4387e+05  rho_loss = 6.5681e+04  epoch time = 48.81s\n",
            "epoch =  167  train_loss = 1.2153e+06  val_loss = 1.2312e+06  u_vert_loss = 6.1705e+05  u_hori_loss = 5.5025e+05  rho_loss = 6.3903e+04  epoch time = 48.41s\n",
            "epoch =  168  train_loss = 1.2152e+06  val_loss = 1.2391e+06  u_vert_loss = 6.1168e+05  u_hori_loss = 5.5549e+05  rho_loss = 7.1924e+04  epoch time = 48.32s\n",
            "epoch =  169  train_loss = 1.2150e+06  val_loss = 1.2256e+06  u_vert_loss = 6.0714e+05  u_hori_loss = 5.4245e+05  rho_loss = 7.6059e+04  epoch time = 49.20s\n",
            "epoch =  170  train_loss = 1.2145e+06  val_loss = 1.2396e+06  u_vert_loss = 6.1659e+05  u_hori_loss = 5.5507e+05  rho_loss = 6.7989e+04  epoch time = 48.37s\n",
            "epoch =  171  train_loss = 1.2145e+06  val_loss = 1.2249e+06  u_vert_loss = 6.1066e+05  u_hori_loss = 5.3943e+05  rho_loss = 7.4849e+04  epoch time = 48.78s\n",
            "epoch =  172  train_loss = 1.2132e+06  val_loss = 1.2418e+06  u_vert_loss = 6.1771e+05  u_hori_loss = 5.5276e+05  rho_loss = 7.1287e+04  epoch time = 48.39s\n",
            "epoch =  173  train_loss = 1.2125e+06  val_loss = 1.2124e+06  u_vert_loss = 6.0433e+05  u_hori_loss = 5.3946e+05  rho_loss = 6.8627e+04  epoch time = 48.44s\n",
            "epoch =  174  train_loss = 1.2134e+06  val_loss = 1.2050e+06  u_vert_loss = 6.0330e+05  u_hori_loss = 5.3297e+05  rho_loss = 6.8765e+04  epoch time = 49.45s\n",
            "epoch =  175  train_loss = 1.2139e+06  val_loss = 1.2236e+06  u_vert_loss = 5.9921e+05  u_hori_loss = 5.5447e+05  rho_loss = 6.9919e+04  epoch time = 48.63s\n",
            "epoch =  176  train_loss = 1.2117e+06  val_loss = 1.1895e+06  u_vert_loss = 5.9144e+05  u_hori_loss = 5.2910e+05  rho_loss = 6.8967e+04  epoch time = 49.13s\n",
            "epoch =  177  train_loss = 1.2135e+06  val_loss = 1.2072e+06  u_vert_loss = 6.0010e+05  u_hori_loss = 5.3609e+05  rho_loss = 7.1003e+04  epoch time = 48.71s\n",
            "epoch =  178  train_loss = 1.2106e+06  val_loss = 1.2149e+06  u_vert_loss = 6.1197e+05  u_hori_loss = 5.3434e+05  rho_loss = 6.8561e+04  epoch time = 48.68s\n",
            "epoch =  179  train_loss = 1.2088e+06  val_loss = 1.2021e+06  u_vert_loss = 5.9696e+05  u_hori_loss = 5.3559e+05  rho_loss = 6.9511e+04  epoch time = 49.49s\n",
            "epoch =  180  train_loss = 1.2103e+06  val_loss = 1.2252e+06  u_vert_loss = 6.1140e+05  u_hori_loss = 5.4582e+05  rho_loss = 6.8021e+04  epoch time = 48.58s\n",
            "epoch =  181  train_loss = 1.2080e+06  val_loss = 1.2343e+06  u_vert_loss = 6.0456e+05  u_hori_loss = 5.5680e+05  rho_loss = 7.2968e+04  epoch time = 48.98s\n",
            "epoch =  182  train_loss = 1.2070e+06  val_loss = 1.2236e+06  u_vert_loss = 6.0391e+05  u_hori_loss = 5.4046e+05  rho_loss = 7.9187e+04  epoch time = 48.66s\n",
            "epoch =  183  train_loss = 1.2077e+06  val_loss = 1.2135e+06  u_vert_loss = 6.0580e+05  u_hori_loss = 5.3713e+05  rho_loss = 7.0586e+04  epoch time = 48.53s\n",
            "epoch =  184  train_loss = 1.2085e+06  val_loss = 1.2299e+06  u_vert_loss = 6.1026e+05  u_hori_loss = 5.5005e+05  rho_loss = 6.9544e+04  epoch time = 49.46s\n",
            "epoch =  185  train_loss = 1.2078e+06  val_loss = 1.2548e+06  u_vert_loss = 6.2923e+05  u_hori_loss = 5.5878e+05  rho_loss = 6.6805e+04  epoch time = 48.58s\n",
            "epoch =  186  train_loss = 1.2083e+06  val_loss = 1.2338e+06  u_vert_loss = 6.1583e+05  u_hori_loss = 5.5165e+05  rho_loss = 6.6358e+04  epoch time = 49.03s\n",
            "epoch =  187  train_loss = 1.2067e+06  val_loss = 1.2669e+06  u_vert_loss = 6.2060e+05  u_hori_loss = 5.6380e+05  rho_loss = 8.2473e+04  epoch time = 48.58s\n",
            "epoch =  188  train_loss = 1.2099e+06  val_loss = 1.2157e+06  u_vert_loss = 6.0648e+05  u_hori_loss = 5.4156e+05  rho_loss = 6.7690e+04  epoch time = 48.66s\n",
            "epoch =  189  train_loss = 1.2075e+06  val_loss = 1.1923e+06  u_vert_loss = 5.8945e+05  u_hori_loss = 5.3602e+05  rho_loss = 6.6801e+04  epoch time = 49.45s\n",
            "epoch =  190  train_loss = 1.2095e+06  val_loss = 1.2127e+06  u_vert_loss = 6.0398e+05  u_hori_loss = 5.4178e+05  rho_loss = 6.6982e+04  epoch time = 48.68s\n",
            "epoch =  191  train_loss = 1.2060e+06  val_loss = 1.2142e+06  u_vert_loss = 6.0205e+05  u_hori_loss = 5.4728e+05  rho_loss = 6.4859e+04  epoch time = 48.93s\n",
            "epoch =  192  train_loss = 1.2056e+06  val_loss = 1.2035e+06  u_vert_loss = 5.9608e+05  u_hori_loss = 5.3946e+05  rho_loss = 6.7964e+04  epoch time = 48.66s\n",
            "epoch =  193  train_loss = 1.2049e+06  val_loss = 1.2212e+06  u_vert_loss = 6.1289e+05  u_hori_loss = 5.3748e+05  rho_loss = 7.0804e+04  epoch time = 48.64s\n",
            "epoch =  194  train_loss = 1.2052e+06  val_loss = 1.2146e+06  u_vert_loss = 6.0185e+05  u_hori_loss = 5.3548e+05  rho_loss = 7.7306e+04  epoch time = 49.23s\n",
            "epoch =  195  train_loss = 1.2055e+06  val_loss = 1.2086e+06  u_vert_loss = 6.0055e+05  u_hori_loss = 5.3355e+05  rho_loss = 7.4527e+04  epoch time = 48.45s\n",
            "epoch =  196  train_loss = 1.2043e+06  val_loss = 1.2266e+06  u_vert_loss = 6.1420e+05  u_hori_loss = 5.4143e+05  rho_loss = 7.1003e+04  epoch time = 48.83s\n",
            "epoch =  197  train_loss = 1.2025e+06  val_loss = 1.2122e+06  u_vert_loss = 5.9458e+05  u_hori_loss = 5.4422e+05  rho_loss = 7.3443e+04  epoch time = 48.27s\n",
            "epoch =  198  train_loss = 1.2041e+06  val_loss = 1.2076e+06  u_vert_loss = 5.9696e+05  u_hori_loss = 5.4421e+05  rho_loss = 6.6378e+04  epoch time = 48.84s\n",
            "epoch =  199  train_loss = 1.2023e+06  val_loss = 1.2304e+06  u_vert_loss = 6.1636e+05  u_hori_loss = 5.4690e+05  rho_loss = 6.7166e+04  epoch time = 48.78s\n",
            "epoch =  200  train_loss = 1.2027e+06  val_loss = 1.2114e+06  u_vert_loss = 6.0536e+05  u_hori_loss = 5.3826e+05  rho_loss = 6.7790e+04  epoch time = 48.31s\n",
            "epoch =  201  train_loss = 1.2013e+06  val_loss = 1.2147e+06  u_vert_loss = 6.0202e+05  u_hori_loss = 5.4128e+05  rho_loss = 7.1436e+04  epoch time = 48.71s\n",
            "epoch =  202  train_loss = 1.2033e+06  val_loss = 1.2100e+06  u_vert_loss = 6.0366e+05  u_hori_loss = 5.3715e+05  rho_loss = 6.9235e+04  epoch time = 48.26s\n",
            "epoch =  203  train_loss = 1.2017e+06  val_loss = 1.1990e+06  u_vert_loss = 5.9760e+05  u_hori_loss = 5.3565e+05  rho_loss = 6.5735e+04  epoch time = 48.45s\n",
            "epoch =  204  train_loss = 1.2005e+06  val_loss = 1.2158e+06  u_vert_loss = 6.0410e+05  u_hori_loss = 5.4468e+05  rho_loss = 6.6990e+04  epoch time = 49.21s\n",
            "epoch =  205  train_loss = 1.2017e+06  val_loss = 1.1931e+06  u_vert_loss = 5.9401e+05  u_hori_loss = 5.3157e+05  rho_loss = 6.7546e+04  epoch time = 48.36s\n",
            "epoch =  206  train_loss = 1.1998e+06  val_loss = 1.2103e+06  u_vert_loss = 6.0188e+05  u_hori_loss = 5.3521e+05  rho_loss = 7.3165e+04  epoch time = 48.82s\n",
            "epoch =  207  train_loss = 1.2008e+06  val_loss = 1.2045e+06  u_vert_loss = 5.9449e+05  u_hori_loss = 5.3460e+05  rho_loss = 7.5440e+04  epoch time = 48.34s\n",
            "epoch =  208  train_loss = 1.2003e+06  val_loss = 1.1915e+06  u_vert_loss = 5.9184e+05  u_hori_loss = 5.3238e+05  rho_loss = 6.7326e+04  epoch time = 48.89s\n",
            "epoch =  209  train_loss = 1.1984e+06  val_loss = 1.2340e+06  u_vert_loss = 6.1186e+05  u_hori_loss = 5.4294e+05  rho_loss = 7.9169e+04  epoch time = 48.99s\n",
            "epoch =  210  train_loss = 1.1971e+06  val_loss = 1.2430e+06  u_vert_loss = 6.0394e+05  u_hori_loss = 5.4907e+05  rho_loss = 9.0023e+04  epoch time = 48.43s\n",
            "epoch =  211  train_loss = 1.1981e+06  val_loss = 1.1839e+06  u_vert_loss = 5.8904e+05  u_hori_loss = 5.2890e+05  rho_loss = 6.5992e+04  epoch time = 48.90s\n",
            "epoch =  212  train_loss = 1.1963e+06  val_loss = 1.1960e+06  u_vert_loss = 5.8914e+05  u_hori_loss = 5.3490e+05  rho_loss = 7.1991e+04  epoch time = 48.48s\n",
            "epoch =  213  train_loss = 1.1961e+06  val_loss = 1.2300e+06  u_vert_loss = 6.0603e+05  u_hori_loss = 5.5385e+05  rho_loss = 7.0167e+04  epoch time = 48.88s\n",
            "epoch =  214  train_loss = 1.1971e+06  val_loss = 1.1893e+06  u_vert_loss = 5.9197e+05  u_hori_loss = 5.2970e+05  rho_loss = 6.7591e+04  epoch time = 49.14s\n",
            "epoch =  215  train_loss = 1.1954e+06  val_loss = 1.1820e+06  u_vert_loss = 5.8693e+05  u_hori_loss = 5.2776e+05  rho_loss = 6.7345e+04  epoch time = 48.93s\n",
            "epoch =  216  train_loss = 1.1971e+06  val_loss = 1.1874e+06  u_vert_loss = 5.8555e+05  u_hori_loss = 5.3147e+05  rho_loss = 7.0327e+04  epoch time = 48.61s\n",
            "epoch =  217  train_loss = 1.1954e+06  val_loss = 1.1957e+06  u_vert_loss = 5.9214e+05  u_hori_loss = 5.3359e+05  rho_loss = 6.9962e+04  epoch time = 49.07s\n",
            "epoch =  218  train_loss = 1.1961e+06  val_loss = 1.2152e+06  u_vert_loss = 6.0365e+05  u_hori_loss = 5.3612e+05  rho_loss = 7.5414e+04  epoch time = 48.59s\n",
            "epoch =  219  train_loss = 1.1947e+06  val_loss = 1.1981e+06  u_vert_loss = 5.9802e+05  u_hori_loss = 5.3339e+05  rho_loss = 6.6677e+04  epoch time = 49.10s\n",
            "epoch =  220  train_loss = 1.1980e+06  val_loss = 1.1953e+06  u_vert_loss = 5.9411e+05  u_hori_loss = 5.3622e+05  rho_loss = 6.4932e+04  epoch time = 48.75s\n",
            "epoch =  221  train_loss = 1.1933e+06  val_loss = 1.2041e+06  u_vert_loss = 5.9872e+05  u_hori_loss = 5.3595e+05  rho_loss = 6.9397e+04  epoch time = 48.23s\n",
            "epoch =  222  train_loss = 1.1947e+06  val_loss = 1.2041e+06  u_vert_loss = 5.9353e+05  u_hori_loss = 5.3279e+05  rho_loss = 7.7798e+04  epoch time = 48.64s\n",
            "epoch =  223  train_loss = 1.1936e+06  val_loss = 1.2075e+06  u_vert_loss = 5.9786e+05  u_hori_loss = 5.3841e+05  rho_loss = 7.1189e+04  epoch time = 48.32s\n",
            "epoch =  224  train_loss = 1.1954e+06  val_loss = 1.1919e+06  u_vert_loss = 5.9414e+05  u_hori_loss = 5.3360e+05  rho_loss = 6.4139e+04  epoch time = 49.47s\n",
            "epoch =  225  train_loss = 1.1958e+06  val_loss = 1.2029e+06  u_vert_loss = 6.0316e+05  u_hori_loss = 5.3384e+05  rho_loss = 6.5887e+04  epoch time = 48.41s\n",
            "epoch =  226  train_loss = 1.1920e+06  val_loss = 1.1915e+06  u_vert_loss = 5.8961e+05  u_hori_loss = 5.3458e+05  rho_loss = 6.7293e+04  epoch time = 48.22s\n",
            "epoch =  227  train_loss = 1.1929e+06  val_loss = 1.2024e+06  u_vert_loss = 5.9710e+05  u_hori_loss = 5.3518e+05  rho_loss = 7.0084e+04  epoch time = 48.62s\n",
            "epoch =  228  train_loss = 1.1910e+06  val_loss = 1.2087e+06  u_vert_loss = 6.0471e+05  u_hori_loss = 5.3747e+05  rho_loss = 6.6546e+04  epoch time = 48.29s\n",
            "epoch =  229  train_loss = 1.1923e+06  val_loss = 1.1758e+06  u_vert_loss = 5.8179e+05  u_hori_loss = 5.3089e+05  rho_loss = 6.3132e+04  epoch time = 49.04s\n",
            "epoch =  230  train_loss = 1.1921e+06  val_loss = 1.2160e+06  u_vert_loss = 5.9858e+05  u_hori_loss = 5.4781e+05  rho_loss = 6.9611e+04  epoch time = 48.32s\n",
            "epoch =  231  train_loss = 1.1911e+06  val_loss = 1.2035e+06  u_vert_loss = 5.9479e+05  u_hori_loss = 5.4058e+05  rho_loss = 6.8108e+04  epoch time = 48.59s\n",
            "epoch =  232  train_loss = 1.1903e+06  val_loss = 1.2201e+06  u_vert_loss = 6.0782e+05  u_hori_loss = 5.4086e+05  rho_loss = 7.1412e+04  epoch time = 48.19s\n",
            "epoch =  233  train_loss = 1.1921e+06  val_loss = 1.2144e+06  u_vert_loss = 5.9648e+05  u_hori_loss = 5.4676e+05  rho_loss = 7.1157e+04  epoch time = 49.12s\n",
            "epoch =  234  train_loss = 1.1923e+06  val_loss = 1.2084e+06  u_vert_loss = 6.0267e+05  u_hori_loss = 5.3504e+05  rho_loss = 7.0735e+04  epoch time = 49.08s\n",
            "epoch =  235  train_loss = 1.1916e+06  val_loss = 1.1964e+06  u_vert_loss = 5.9370e+05  u_hori_loss = 5.3644e+05  rho_loss = 6.6230e+04  epoch time = 48.79s\n",
            "epoch =  236  train_loss = 1.1893e+06  val_loss = 1.1983e+06  u_vert_loss = 5.9462e+05  u_hori_loss = 5.3756e+05  rho_loss = 6.6121e+04  epoch time = 48.72s\n",
            "epoch =  237  train_loss = 1.1895e+06  val_loss = 1.2082e+06  u_vert_loss = 6.0390e+05  u_hori_loss = 5.3831e+05  rho_loss = 6.5977e+04  epoch time = 48.31s\n",
            "epoch =  238  train_loss = 1.1885e+06  val_loss = 1.1817e+06  u_vert_loss = 5.9004e+05  u_hori_loss = 5.2837e+05  rho_loss = 6.3262e+04  epoch time = 48.63s\n",
            "epoch =  239  train_loss = 1.1889e+06  val_loss = 1.2230e+06  u_vert_loss = 6.0019e+05  u_hori_loss = 5.4652e+05  rho_loss = 7.6288e+04  epoch time = 48.72s\n",
            "epoch =  240  train_loss = 1.1909e+06  val_loss = 1.2142e+06  u_vert_loss = 6.0097e+05  u_hori_loss = 5.4229e+05  rho_loss = 7.0908e+04  epoch time = 48.69s\n",
            "epoch =  241  train_loss = 1.1885e+06  val_loss = 1.2007e+06  u_vert_loss = 6.0837e+05  u_hori_loss = 5.2543e+05  rho_loss = 6.6875e+04  epoch time = 48.40s\n",
            "epoch =  242  train_loss = 1.1883e+06  val_loss = 1.1995e+06  u_vert_loss = 5.9544e+05  u_hori_loss = 5.3261e+05  rho_loss = 7.1411e+04  epoch time = 48.93s\n",
            "epoch =  243  train_loss = 1.1886e+06  val_loss = 1.2038e+06  u_vert_loss = 5.9445e+05  u_hori_loss = 5.3523e+05  rho_loss = 7.4137e+04  epoch time = 48.37s\n",
            "epoch =  244  train_loss = 1.1882e+06  val_loss = 1.1951e+06  u_vert_loss = 5.9374e+05  u_hori_loss = 5.3318e+05  rho_loss = 6.8224e+04  epoch time = 48.75s\n",
            "epoch =  245  train_loss = 1.1877e+06  val_loss = 1.2192e+06  u_vert_loss = 6.0822e+05  u_hori_loss = 5.3453e+05  rho_loss = 7.6435e+04  epoch time = 48.63s\n",
            "epoch =  246  train_loss = 1.1894e+06  val_loss = 1.2047e+06  u_vert_loss = 5.9762e+05  u_hori_loss = 5.3869e+05  rho_loss = 6.8370e+04  epoch time = 48.26s\n",
            "epoch =  247  train_loss = 1.1866e+06  val_loss = 1.2129e+06  u_vert_loss = 6.0059e+05  u_hori_loss = 5.4073e+05  rho_loss = 7.1539e+04  epoch time = 48.61s\n",
            "epoch =  248  train_loss = 1.1878e+06  val_loss = 1.2053e+06  u_vert_loss = 5.9956e+05  u_hori_loss = 5.3794e+05  rho_loss = 6.7825e+04  epoch time = 48.29s\n",
            "epoch =  249  train_loss = 1.1858e+06  val_loss = 1.2049e+06  u_vert_loss = 6.0157e+05  u_hori_loss = 5.3442e+05  rho_loss = 6.8958e+04  epoch time = 49.26s\n",
            "epoch =  250  train_loss = 1.1864e+06  val_loss = 1.2079e+06  u_vert_loss = 6.0209e+05  u_hori_loss = 5.3288e+05  rho_loss = 7.2946e+04  epoch time = 48.23s\n",
            "epoch =  251  train_loss = 1.1866e+06  val_loss = 1.2096e+06  u_vert_loss = 6.0228e+05  u_hori_loss = 5.3897e+05  rho_loss = 6.8392e+04  epoch time = 49.00s\n",
            "epoch =  252  train_loss = 1.1852e+06  val_loss = 1.2182e+06  u_vert_loss = 6.0470e+05  u_hori_loss = 5.3944e+05  rho_loss = 7.4027e+04  epoch time = 48.14s\n",
            "epoch =  253  train_loss = 1.1862e+06  val_loss = 1.1907e+06  u_vert_loss = 5.8927e+05  u_hori_loss = 5.3033e+05  rho_loss = 7.1091e+04  epoch time = 48.30s\n",
            "epoch =  254  train_loss = 1.1843e+06  val_loss = 1.1965e+06  u_vert_loss = 5.8599e+05  u_hori_loss = 5.4607e+05  rho_loss = 6.4387e+04  epoch time = 49.03s\n",
            "epoch =  255  train_loss = 1.1846e+06  val_loss = 1.2012e+06  u_vert_loss = 5.9641e+05  u_hori_loss = 5.3704e+05  rho_loss = 6.7797e+04  epoch time = 48.35s\n",
            "epoch =  256  train_loss = 1.1837e+06  val_loss = 1.1960e+06  u_vert_loss = 5.9304e+05  u_hori_loss = 5.3068e+05  rho_loss = 7.2306e+04  epoch time = 48.95s\n",
            "epoch =  257  train_loss = 1.1858e+06  val_loss = 1.2190e+06  u_vert_loss = 6.0679e+05  u_hori_loss = 5.4413e+05  rho_loss = 6.8086e+04  epoch time = 48.52s\n",
            "epoch =  258  train_loss = 1.1820e+06  val_loss = 1.2160e+06  u_vert_loss = 6.0337e+05  u_hori_loss = 5.3864e+05  rho_loss = 7.4014e+04  epoch time = 49.02s\n",
            "epoch =  259  train_loss = 1.1838e+06  val_loss = 1.1795e+06  u_vert_loss = 5.8394e+05  u_hori_loss = 5.2846e+05  rho_loss = 6.7120e+04  epoch time = 49.44s\n",
            "epoch =  260  train_loss = 1.1844e+06  val_loss = 1.1949e+06  u_vert_loss = 5.8935e+05  u_hori_loss = 5.3069e+05  rho_loss = 7.4908e+04  epoch time = 48.91s\n",
            "epoch =  261  train_loss = 1.1828e+06  val_loss = 1.1873e+06  u_vert_loss = 5.9066e+05  u_hori_loss = 5.2851e+05  rho_loss = 6.8143e+04  epoch time = 48.69s\n",
            "epoch =  262  train_loss = 1.1841e+06  val_loss = 1.2228e+06  u_vert_loss = 6.0488e+05  u_hori_loss = 5.4472e+05  rho_loss = 7.3180e+04  epoch time = 49.54s\n",
            "epoch =  263  train_loss = 1.1835e+06  val_loss = 1.2060e+06  u_vert_loss = 5.9544e+05  u_hori_loss = 5.3769e+05  rho_loss = 7.2834e+04  epoch time = 48.54s\n",
            "epoch =  264  train_loss = 1.1828e+06  val_loss = 1.1880e+06  u_vert_loss = 5.9216e+05  u_hori_loss = 5.3095e+05  rho_loss = 6.4866e+04  epoch time = 49.05s\n",
            "epoch =  265  train_loss = 1.1811e+06  val_loss = 1.1907e+06  u_vert_loss = 5.8860e+05  u_hori_loss = 5.2992e+05  rho_loss = 7.2230e+04  epoch time = 48.90s\n",
            "epoch =  266  train_loss = 1.1824e+06  val_loss = 1.1995e+06  u_vert_loss = 5.9470e+05  u_hori_loss = 5.3481e+05  rho_loss = 7.0006e+04  epoch time = 48.55s\n",
            "epoch =  267  train_loss = 1.1818e+06  val_loss = 1.1874e+06  u_vert_loss = 5.9255e+05  u_hori_loss = 5.3552e+05  rho_loss = 5.9303e+04  epoch time = 49.42s\n",
            "epoch =  268  train_loss = 1.1798e+06  val_loss = 1.1867e+06  u_vert_loss = 5.8893e+05  u_hori_loss = 5.2460e+05  rho_loss = 7.3155e+04  epoch time = 49.06s\n",
            "epoch =  269  train_loss = 1.1804e+06  val_loss = 1.1990e+06  u_vert_loss = 5.9692e+05  u_hori_loss = 5.2969e+05  rho_loss = 7.2368e+04  epoch time = 49.65s\n",
            "epoch =  270  train_loss = 1.1831e+06  val_loss = 1.2024e+06  u_vert_loss = 5.9521e+05  u_hori_loss = 5.3926e+05  rho_loss = 6.7956e+04  epoch time = 48.49s\n",
            "epoch =  271  train_loss = 1.1804e+06  val_loss = 1.2089e+06  u_vert_loss = 6.0323e+05  u_hori_loss = 5.3550e+05  rho_loss = 7.0211e+04  epoch time = 48.85s\n",
            "epoch =  272  train_loss = 1.1829e+06  val_loss = 1.2134e+06  u_vert_loss = 6.0024e+05  u_hori_loss = 5.3661e+05  rho_loss = 7.6529e+04  epoch time = 48.57s\n",
            "epoch =  273  train_loss = 1.1796e+06  val_loss = 1.2292e+06  u_vert_loss = 6.0839e+05  u_hori_loss = 5.4795e+05  rho_loss = 7.2847e+04  epoch time = 48.79s\n",
            "epoch =  274  train_loss = 1.1808e+06  val_loss = 1.1949e+06  u_vert_loss = 5.9825e+05  u_hori_loss = 5.2892e+05  rho_loss = 6.7704e+04  epoch time = 48.94s\n",
            "epoch =  275  train_loss = 1.1811e+06  val_loss = 1.1979e+06  u_vert_loss = 5.9395e+05  u_hori_loss = 5.3798e+05  rho_loss = 6.6011e+04  epoch time = 48.94s\n",
            "epoch =  276  train_loss = 1.1782e+06  val_loss = 1.1898e+06  u_vert_loss = 5.9586e+05  u_hori_loss = 5.2939e+05  rho_loss = 6.4560e+04  epoch time = 48.43s\n",
            "epoch =  277  train_loss = 1.5175e+06  val_loss = 1.2005e+06  u_vert_loss = 5.8908e+05  u_hori_loss = 5.3462e+05  rho_loss = 7.6780e+04  epoch time = 48.72s\n",
            "epoch =  278  train_loss = 1.1819e+06  val_loss = 1.1778e+06  u_vert_loss = 5.8404e+05  u_hori_loss = 5.3172e+05  rho_loss = 6.2049e+04  epoch time = 48.35s\n",
            "epoch =  279  train_loss = 1.1782e+06  val_loss = 1.2191e+06  u_vert_loss = 5.9967e+05  u_hori_loss = 5.5269e+05  rho_loss = 6.6723e+04  epoch time = 49.16s\n",
            "epoch =  280  train_loss = 1.1786e+06  val_loss = 1.1867e+06  u_vert_loss = 5.8895e+05  u_hori_loss = 5.3308e+05  rho_loss = 6.4710e+04  epoch time = 48.37s\n",
            "epoch =  281  train_loss = 1.1791e+06  val_loss = 1.1914e+06  u_vert_loss = 5.8910e+05  u_hori_loss = 5.3514e+05  rho_loss = 6.7173e+04  epoch time = 49.12s\n",
            "epoch =  282  train_loss = 1.1775e+06  val_loss = 1.1996e+06  u_vert_loss = 5.9589e+05  u_hori_loss = 5.3074e+05  rho_loss = 7.3001e+04  epoch time = 48.39s\n",
            "epoch =  283  train_loss = 1.1805e+06  val_loss = 1.1824e+06  u_vert_loss = 5.8454e+05  u_hori_loss = 5.2876e+05  rho_loss = 6.9075e+04  epoch time = 48.51s\n",
            "epoch =  284  train_loss = 1.1787e+06  val_loss = 1.2046e+06  u_vert_loss = 5.8333e+05  u_hori_loss = 5.3842e+05  rho_loss = 8.2886e+04  epoch time = 49.22s\n",
            "epoch =  285  train_loss = 1.1785e+06  val_loss = 1.2048e+06  u_vert_loss = 5.9114e+05  u_hori_loss = 5.4156e+05  rho_loss = 7.2047e+04  epoch time = 48.40s\n",
            "epoch =  286  train_loss = 1.1782e+06  val_loss = 1.1797e+06  u_vert_loss = 5.8713e+05  u_hori_loss = 5.2404e+05  rho_loss = 6.8524e+04  epoch time = 48.75s\n",
            "epoch =  287  train_loss = 1.1775e+06  val_loss = 1.1836e+06  u_vert_loss = 5.9092e+05  u_hori_loss = 5.2854e+05  rho_loss = 6.4144e+04  epoch time = 48.47s\n",
            "epoch =  288  train_loss = 1.1766e+06  val_loss = 1.2067e+06  u_vert_loss = 5.9348e+05  u_hori_loss = 5.4145e+05  rho_loss = 7.1804e+04  epoch time = 48.78s\n",
            "epoch =  289  train_loss = 1.1762e+06  val_loss = 1.1869e+06  u_vert_loss = 5.9565e+05  u_hori_loss = 5.2294e+05  rho_loss = 6.8314e+04  epoch time = 48.91s\n",
            "epoch =  290  train_loss = 1.1766e+06  val_loss = 1.1920e+06  u_vert_loss = 5.9274e+05  u_hori_loss = 5.2720e+05  rho_loss = 7.2088e+04  epoch time = 48.73s\n",
            "epoch =  291  train_loss = 1.1766e+06  val_loss = 1.1856e+06  u_vert_loss = 5.9367e+05  u_hori_loss = 5.2594e+05  rho_loss = 6.5989e+04  epoch time = 48.45s\n",
            "epoch =  292  train_loss = 1.1771e+06  val_loss = 1.1825e+06  u_vert_loss = 5.8971e+05  u_hori_loss = 5.2429e+05  rho_loss = 6.8493e+04  epoch time = 48.72s\n",
            "epoch =  293  train_loss = 1.1747e+06  val_loss = 1.1944e+06  u_vert_loss = 5.9373e+05  u_hori_loss = 5.3092e+05  rho_loss = 6.9755e+04  epoch time = 48.49s\n",
            "epoch =  294  train_loss = 1.1763e+06  val_loss = 1.2086e+06  u_vert_loss = 6.0531e+05  u_hori_loss = 5.3798e+05  rho_loss = 6.5286e+04  epoch time = 49.29s\n",
            "epoch =  295  train_loss = 1.1733e+06  val_loss = 1.1890e+06  u_vert_loss = 5.9317e+05  u_hori_loss = 5.2987e+05  rho_loss = 6.5938e+04  epoch time = 48.34s\n",
            "epoch =  296  train_loss = 1.1734e+06  val_loss = 1.1795e+06  u_vert_loss = 5.8716e+05  u_hori_loss = 5.2880e+05  rho_loss = 6.3532e+04  epoch time = 48.80s\n",
            "epoch =  297  train_loss = 1.1762e+06  val_loss = 1.1872e+06  u_vert_loss = 5.9113e+05  u_hori_loss = 5.2279e+05  rho_loss = 7.3287e+04  epoch time = 48.45s\n",
            "epoch =  298  train_loss = 1.1742e+06  val_loss = 1.1828e+06  u_vert_loss = 5.8608e+05  u_hori_loss = 5.2818e+05  rho_loss = 6.8588e+04  epoch time = 48.79s\n",
            "epoch =  299  train_loss = 1.1726e+06  val_loss = 1.1899e+06  u_vert_loss = 5.9286e+05  u_hori_loss = 5.2639e+05  rho_loss = 7.0674e+04  epoch time = 48.88s\n",
            "Done\n"
          ]
        }
      ],
      "source": [
        "# todo: enable the model to receive batch data\n",
        "exec_datetime = dt.datetime.now()\n",
        "log_dir = f'/content/drive/MyDrive/lab_data/logs/{exec_datetime.strftime(\"%Y%m%d_%H%M%S\")}'\n",
        "writer = SummaryWriter(log_dir=log_dir)\n",
        "\n",
        "def main(dataset, train_loader, valid_loader, writer: SummaryWriter):\n",
        "    print('Calc losses between 0h and 3h...')\n",
        "    losses_0h3h = calc_losses_between_0h3h(train_loader, valid_loader)\n",
        "    print(f'train u_vert mae: {losses_0h3h.get_losses_train()[\"u_vert_mae\"]} m/s')\n",
        "    print(f'train u_hori mae: {losses_0h3h.get_losses_train()[\"u_hori_mae\"]} m/s')\n",
        "    print(f'train rho mae: {losses_0h3h.get_losses_train()[\"rho_mae\"]} hPa')\n",
        "    print(f'val u_vert mae: {losses_0h3h.get_losses_val()[\"u_vert_mae\"]} m/s')\n",
        "    print(f'val u_hori mae: {losses_0h3h.get_losses_val()[\"u_hori_mae\"]} m/s')\n",
        "    print(f'val rho mae: {losses_0h3h.get_losses_val()[\"rho_mae\"]} hPa')\n",
        "    print('Creating the model...')\n",
        "    lbm = LBMwithNoisyInit(HEIGHT, WIDTH).to(device)\n",
        "    weight_diff_logger = WeightDiffLogger()\n",
        "\n",
        "    num_epochs = 300\n",
        "    total_time = 0.0\n",
        "    \n",
        "\n",
        "    optimizer = torch.optim.Adam(lbm.parameters())\n",
        "    # optimizer = torch.optim.Adadelta(lbm.parameters())\n",
        "    # optimizer = torch.optim.AdamW(lbm.parameters())\n",
        "    # optimizer = torch.optim.SGD(lbm.parameters(), lr=0.001, momentum=0.9, nesterov=True)\n",
        "\n",
        "    \n",
        "    print(\"optimizer = \" + str(optimizer))\n",
        "    print(\"max_epochs = %3d \" % num_epochs)\n",
        "    print(\"summary writer's logdir = \" + writer.log_dir )\n",
        "    \n",
        "    print('Training the model...')\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        lbm.train()\n",
        "        epoch_loss = Loss()\n",
        "        epoch_started_time = time.time()\n",
        "        weight_diff_logger.memorize(lbm)\n",
        "        for (batch_idx, batch) in enumerate(train_loader):\n",
        "            optimizer.zero_grad()\n",
        "            out = lbm(batch['u_vert'].squeeze(), batch['u_hori'].squeeze(), batch['rho'].squeeze())\n",
        "            loss_vals = loss_func(out, batch)\n",
        "            epoch_loss.add_losses_train(loss_vals)\n",
        "            log_typhoon_forcast(out, batch, writer, epoch)\n",
        "            loss_vals['total'].backward()\n",
        "            optimizer.step()\n",
        "        \n",
        "        lbm.eval()\n",
        "        with torch.no_grad():\n",
        "            for (batch_idx, batch) in enumerate(valid_loader):\n",
        "                out = lbm(batch['u_vert'].squeeze(), batch['u_hori'].squeeze(), batch['rho'].squeeze())\n",
        "                loss_vals = loss_func(out, batch)\n",
        "                epoch_loss.add_losses_val(loss_vals)\n",
        "                log_typhoon_forcast(out, batch, writer, epoch)\n",
        "\n",
        "        losses_train = epoch_loss.get_losses_train()\n",
        "        losses_val = epoch_loss.get_losses_val()\n",
        "        weight_diff_logger.calc_and_log(lbm, writer, epoch)\n",
        "        print(\"epoch = %4d  train_loss = %.4e  val_loss = %.4e  u_vert_loss = %.4e  u_hori_loss = %.4e  rho_loss = %.4e  epoch time = %0.2fs\"\\\n",
        "              % (epoch, losses_train['total'], losses_val['total'], losses_val['u_vert'], losses_val['u_hori'], losses_val['rho'], time.time() - epoch_started_time))\n",
        "        epoch_loss.log(writer, epoch, 'MSE_loss', ['total', 'u_vert', 'u_hori', 'rho'])\n",
        "        epoch_loss.log(writer, epoch, 'MAE', ['u_vert_mae', 'u_hori_mae', 'rho_mae'])\n",
        "        losses_0h3h.log(writer, epoch, 'MAE', ['u_vert_mae', 'u_hori_mae', 'rho_mae'], prefix='0h3h')\n",
        "\n",
        "\n",
        "    print('Done')\n",
        "        \n",
        "main(dataset, train_loader, valid_loader, writer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-d32fgxFPDq"
      },
      "source": [
        "# Tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X3CZN_v2AY53"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir=$log_dir\n",
        "# %tensorboard --logdir=/content/drive/MyDrive/lab_data/logs/20221230_123857"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxFyExpF1jlh"
      },
      "source": [
        "# Research Diary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axe1JazxpFDn"
      },
      "source": [
        "## 2022/12/31"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uw_FEwispIwd"
      },
      "source": [
        "### やったこと\n",
        "constのU_COEFを色々変えてみた\n",
        "\n",
        "### わかったこと\n",
        "U_COEF = 0.0025のときが一番よかった\n",
        "\n",
        "爆発せず、一番はやく収束した\n",
        "今後しばらくこの値を使ってやってみる\n",
        "\n",
        "### 次にやること"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsj35AVCFjkq"
      },
      "source": [
        "## 2022/12/25"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daTl7brokIvn"
      },
      "source": [
        "\n",
        "### したこと\n",
        "- 層をもう一層ふやして、streamingを5層、collidingを4層にした\n",
        "  - 若干精度良くなった気がする？その前に6.5層でためしたら収束しなくなってしまったので、層を減らしてみた\n",
        "- なんか別に勾配消失がおこっているわけでもなく、むしろ勾配爆発しているようなきもする？\n",
        "  - optimizerをAdams(lrなどはデフォルト値)とかでやってみた結果\n",
        "    - <img src=\"https://drive.google.com/uc?id=1egCA3TmOssUKLbXI39WgBf77S0KxHdOC\" width='400px'>\n",
        "- なので、この後に勾配爆発しないような対策を考える必要がある\n",
        "  1. 勾配を制限する\n",
        "  1. u_vert, u_hori, rhoそれぞれの係数を再度検討する。きちんと数値的に分析して係数変化させてみたほうが良さそう。\n",
        "    - それから、係数がマジックナンバーになっているので、定数化したほうがよいだろう\n",
        "  1. [parameterごとにlrが設定できるらしい](https://pystyle.info/pytorch-sgd/#:~:text=17-,%E3%83%91%E3%83%A9%E3%83%A1%E3%83%BC%E3%82%BF%E3%81%94%E3%81%A8%E3%81%AB%E5%90%84%E7%A8%AE%E4%BF%82%E6%95%B0%E3%82%92%E5%A4%89%E6%9B%B4%E3%81%97%E3%81%9F%E3%81%84%E5%A0%B4%E5%90%88,-torch.optim.SGD)\n",
        "- 現在Adamsでやっているが他のoptimizerも使えないか検証している\n",
        "- リファクタリングしたほうがいい すでにだいぶ読みにくくなっている\n",
        "- なぜか台風の日の画像がすべて取得できていないんだがなぜ…？\n",
        "\n",
        "### 今後の優先順位\n",
        "\n",
        "1. optimizerを変えたときの比較\n",
        "1. リファクタリング\n",
        "  1. u_vert, u_hori, rhoの係数を定数化する\n",
        "  1. main関数のメトリクスを取るところを関数化\n",
        "  1. u_vert, u_horiのMAEをとって、三時間でのMAEも取っておく(グラフで定数値として表しておくとわかりやすいと思う), それからu_squaredとrhoのlossもメトリクスを取っておいたほうがバランスよく学習できているか見やすい気がする\n",
        "  1. 台風の日の画像がすべて取れるようにデバッグ\n",
        "  1. モデルを100epochごとに保存\n",
        "1. 勾配爆発の対策\n",
        "1. パラメータを増やす 特に、(feq + fprev) / 2.0のところをパラメータ化して学習させてみる\n",
        "\n",
        "### 結果\n",
        "1. optimizerを変えたときの画像比較\n",
        "  1. Adam ... 最後に勾配爆発しているが、概ねよく収束している\n",
        "    - <img src=\"https://drive.google.com/uc?id=14VJY4AnRNW-LXsWX3E7q_urOCADuQPxX\" width='200px'> <img src=\"https://drive.google.com/uc?id=1xdePf_J_wcNOAT_GCm2tdmSBnwCiKJzU\" width='200px'>\n",
        "  1. Adadelta ... ダメそう 早々に勾配爆発してしまっているし、収束も悪かった\n",
        "    - <img src=\"https://drive.google.com/uc?id=133r10FBEA7hA-fELX_FJofO_dXc7eV6A\" width='200px'> <img src=\"https://drive.google.com/uc?id=1ezzPaYjngrJ1VC_R59k_NQIh64zL3Jze\" width='200px'>\n",
        "  1. AdamW ... だめっぽい。途中から勾配爆発してしまった\n",
        "    - <img src=\"https://drive.google.com/uc?id=1YTnioqIen_FujQEm8DZIDaq19CunWzh2\" width='200px'> <img src=\"https://drive.google.com/uc?id=1X3kU2nv-EtGdvBbDenXyxnLkVXzZFm4I\" width='200px'>\n",
        "  1. NAG ... なぜかバグってできない...\n",
        "\n",
        "1. リファクタリング\n",
        "  1. u_vert, u_hori, rhoの係数を定数化\n",
        "  1. weightのメトリクスとるところを関数化した\n",
        "  1. 台風の画像取るところを関数化した\n",
        "  1. lossもクラス化して若干みえやすくなった…と思う\n",
        "  1. MAEもとれるようにした。メトリクスが充実してきた感がある\n",
        "  1. 0hと3hのMAEをとって比較できるようにした\n",
        "  1. modelのほぞんは次回\n",
        "\n",
        "rho のMAEがめちゃでかいのがきになる なぜ？\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUeCrHYU1IPn"
      },
      "source": [
        "## 進捗ゼミ 2022/12/21"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPzs3mGHpKRE"
      },
      "source": [
        "### 今回したこと\n",
        "- ここにあるコード全部\n",
        "- PyTorchに移植が完了した！👏\n",
        "  - GPUで速度が爆速に 2層で1epochあたり1時間かかっていたのが、4層まで増やして1epochあたり30秒で終わるようになった\n",
        "  - もう逆誤差伝播を手計算する必要がない(自動微分)\n",
        "  - SGD, Adamなどの最適化関数が用意されているので、それを使うだけ\n",
        "- いままでは出力のu_vert, u_horiしか損失関数に使っていなかったが、その他にもrhoを使うようにした\n",
        "\n",
        "### これからやること\n",
        "\n",
        "- もう少しepoch数を増やして実験(?)\n",
        "- Adam以外の最適化関数の利用、比較検討\n",
        "- 学習済みモデルを都度保存する\n",
        "  - モデルのインスタンスがmain関数に埋め込まれているので、これを外に出す\n",
        "- 重み成分のチェック（勾配が消失していないか確かめる）\n",
        "  - 勾配が消失していないなら層を増やしてみる\n",
        "  - 消失しているなら重みの伝え方をもう少し検討\n",
        "    - Resnetのように入力値を再度足し合わせる\n",
        "    - 線形補間した教師データを用意し、転移学習\n",
        "      - 0時間、3時間、6時間でどれくらい線形的な関係にあるのかを調べる必要がありそう\n",
        "- InputLayerの重みも学習できるようにする\n",
        "- ほかのデータ(たとえば海流など)も使えないか考えてみる"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "mount_file_id": "1PsT-57QVxLNcOleW0DCFaSRD9nonylWI",
      "authorship_tag": "ABX9TyPFZ0WAihrOzaPeTr9EU5MF",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}